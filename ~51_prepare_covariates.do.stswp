// ============================================================================
//
// Cargo -- 1.6.0
//
// ============================================================================
cd ~/Dropbox/Papers/10_WorkInProgress/SoftwareNetworks/Data/Cargo

//
// PREPARE FILES
//

// repositories
insheet using "repositories_Cargo.csv", delimiter(";") names clear
	drop if repoid == "RepoID"
	destring size, replace
	destring numstars, replace
	destring numforks, replace
	destring numwatchers, replace
	destring numcontributors, replace

	destring repoid, replace
save "repositories_Cargo.dta", replace

// projects
insheet using "projects_Cargo.csv", delimiter(";") names clear
	drop if id == "ID"
	destring id, replace
	destring repoid, replace
save "projects_Cargo.dta", replace
	rename id projectid
	keep projectid repoid
	
	drop if repoid == .
save projectid_repoid.dta, replace 

// REPO DEPENDENCIES 
//
// NB (from libraries.io): A repository dependency is a dependency upon a Version from a package manager has been specified in a manifest file, either as a manually added dependency committed by a user or listed as a generated dependency listed in a lockfile that has been automatically generated by a package manager and committed.
insheet using repo_dependencies_Cargo.csv, names delimiter(";") clear
	drop id 
	drop projectname dependencyprojectname
	drop dependencyrequirements
	
	drop if dependencyprojectid == ""
	drop if dependencyprojectid == "DependencyProjectID"
	
	destring dependencyprojectid, replace
	rename dependencyprojectid projectid
	
	// repoid already exists
	rename repoid from_repo_name
	destring from_repo_name, replace
	
	// match dependencyproject
	drop if projectid == "" | projectid == "runtime" | projectid == "]}" | projectid == "=>true" | projectid == "=>false}"
	destring projectid, replace
merge m:1 projectid using projectid_repoid.dta
	keep if _merge == 3
	drop _merge

	rename projectid to_project_name
	rename repoid to_repo_name
	sort from_repo_name to_repo_name
	order from_repo_name to_repo_name
save repo_match_dependencies_Cargo2.dta, replace
	keep to_repo_name from_repo_name
	duplicates drop
outsheet using "dependencies_Cargo-repo2.csv", delimiter(";") nonames replace
save "dependencies_Cargo-repo2.dta", replace // 1,023,801 obs
// RUN ./30_create_dependency_graph-1.6.0.py TO CREATE .GEXF FILE

// INTERLUDE: generate unique IDs for each repo, ensuring that they are the same for both to and from repos
use dependencies_Cargo-repo2.dta, clear
	rename from_repo_name repo_name 
	drop to_repo_name
save tmp_repo_id.dta, replace
use dependencies_Cargo-repo2.dta, clear
	rename to_repo_name repo_name 
	drop from_repo_name 
append using tmp_repo_id.dta
	duplicates drop
	egen id_repo = group(repo_name)
	sort id_repo
save id_repo_name.dta, replace

// MERGE newly generated unique to dependencies
use dependencies_Cargo-repo2.dta, clear
	rename from_repo_name repo_name
merge m:1 repo_name using id_repo_name.dta
	keep if _merge == 3
	drop _merge
	rename repo_name repo_from 
	rename id_repo id_repo_from 

	rename to_repo_name repo_name
merge m:1 repo_name using id_repo_name.dta
	keep if _merge == 3
	drop _merge
	rename repo_name repo_to
	rename id_repo id_repo_to
	
	sort id_repo_from id_repo_to
	order id_repo_from id_repo_to
save dependencies_Cargo-repo3.dta, replace	
outsheet using dependencies_Cargo-repo3.csv, delimiter(";") replace // 102,981 obs


//
// PREPARE CENTRALITIES -- ONCE GEPHI COMPUTES Average Degree, PageRank (0.85, 0.001), Eigenvector Centrality (Directed, 100 iterations)
//
insheet using gephi_analysis_dependencies_Cargo-repo2-lcc.csv, clear nonames
	drop v2 v3
	rename v1 repo_name 
	rename v4 in_degree
	rename v5 out_degree
	drop v6
	rename v7 pagerank
	rename v8 ev_centrality
	sort repo_name
save analysis_dependencies_Cargo-repo2-lcc.dta, replace


//
// START CREATING MASTER DATASET
//
cd ~/Dropbox/Papers/10_WorkInProgress/SoftwareNetworks/Data/Cargo
use repositories_Cargo.dta, clear
	drop description 
	rename repoid repo_name
	sort repo_name

merge 1:1 repo_name using analysis_dependencies_Cargo-repo2-lcc.dta
	keep if _merge == 3 // we lose 22,005 out of 70,090 observations
	drop _merge

	rename size Size
	rename numcontributors NumContributors
	
	drop if Size == 0  // some packages have zero size
	rename numstars Popularity
	drop if Popularity > 50000 // alternative, cuts only the largest value and missing values
	rename numforks NumForks 
	rename numwatchers NumWatchers
save 10_popularity_centrality-repo.dta, replace

//
// INTERLUDE -- ANALYZE MASTER DATA
//
// 	winsor2 ev_centrality, replace cuts(0 95) trim
// 	winsor2 katz_centrality, replace cuts(0 95) trim
// 	winsor2 indeg_centrality, replace cuts(0,95) trim
// 	winsor2 in_degree, replace cuts(0,99) trim
// 	winsor2 out_degree, replace cuts(0,99) trim
// 	winsor2 NumContributors, replace cuts(0 99) trim
	
	// look at most central packages manually
	gsort - ev_centrality 
	
	// simple scatter plot
	scatter Popularity ev_centrality
	graph export cargo_popularity-ev_centrality.jpg, replace
	
// 	scatter Popularity katz_centrality
// 	graph export cargo_popularity-katz_centrality.jpg, replace

	scatter Popularity pagerank
	graph export cargo_popularity-pagerank.jpg, replace
	
	label variable in_degree "in-degree"
	label variable out_degree "out-degree"
	scatter in_degree out_degree 
	graph export cargo_indeg-outdeg.jpg, replace
	
	// binscatter
	winsor2 ev_centrality, replace cuts(0 95) trim
	binscatter Popularity ev_centrality
	graph export cargo_bs_popularity-ev_centrality.jpg, replace
	
// 	binscatter Popularity katz_centrality
// 	graph export pypi_bs_popularity-katz_centrality.jpg, replace
	
	winsor2 pagerank, replace cuts(0 95) trim
	binscatter Popularity pagerank
	graph export pypi_bs_popularity-pagerank.jpg, replace
	
	// number of contributors vs popularity
	winsor2 Popularity, replace cuts(0 99) trim
	binscatter NumContributors Popularity
	graph export cargo_bs_t99_numcontributors_popularity.jpg, replace
	
	binscatter NumContributors ev_centrality
	graph export cargo_bs_t99_numcontributors_ev_centrality.jpg, replace

	binscatter NumContributors pagerank
	graph export cargo_bs_t99_numcontributors_pagerank.jpg, replace

	// regressions
	regress Popularity NumContributors
	regress Popularity ev_centrality

save tmp_10_popularity_centrality-repo.dta, replace


//
// NOW CONTINUE TO CREATE MASTER DATASET FOR CATEGORICAL VARIABLES
//
use 10_popularity_centrality-repo.dta, clear
merge m:1 repo_name using id_repo_name.dta
	keep if _merge == 3
	drop _merge 
	
	local varlist Size Popularity NumForks NumWatchers NumContributors in_degree out_degree pagerank ev_centrality  // Add your variable names here

	foreach var in `varlist' {
		* Create quartiles using the "xtile" command
		xtile quartile = `var', nq(4)

		* Create four categorical variables based on quartiles for the current variable
		gen `var'_1 = (quartile == 1)
		gen `var'_2 = (quartile == 2)
		gen `var'_3 = (quartile == 3)
		gen `var'_4 = (quartile == 4)

		* Drop the temporary "quartile" variable if you don't need it
		drop quartile
	}
	order id_repo repo_name
save 20_master_Cargo.dta, replace
outsheet using 20_master_Cargo.csv, delimiter(";") replace

	
// ============================================================================
//
// Pypi -- 1.6.0
//
// ============================================================================
cd ~/Dropbox/Papers/10_WorkInProgress/SoftwareNetworks/Data/Pypi

//
// PREPARE FILES
//

// repositories
insheet using "repositories_Pypi.csv", delimiter(";") names clear
	drop if repoid == "RepoID"
	destring size, replace
	destring numstars, replace
	destring numforks, replace
	destring numwatchers, replace
	destring numcontributors, replace

	destring repoid, replace
save "repositories_Pypi.dta", replace

// projects
insheet using "projects_Pypi.csv", delimiter(";") names clear
	drop if id == "ID"
	destring id, replace
save "projects_Pypi.dta", replace
	rename id projectid
	keep projectid repoid
	
	drop if repoid == ""
	drop if projectid == 2327647 | projectid == 4745387
	destring repoid, replace
save projectid_repoid.dta, replace 

// REPO DEPENDENCIES 
//
// NB (from libraries.io): A repository dependency is a dependency upon a Version from a package manager has been specified in a manifest file, either as a manually added dependency committed by a user or listed as a generated dependency listed in a lockfile that has been automatically generated by a package manager and committed.
insheet using repo_dependencies_Pypi.csv, names delimiter(";") clear
	drop id 
	drop projectname dependencyprojectname
	drop dependencyrequirements
	
	drop if dependencyprojectid == ""
	drop if dependencyprojectid == "DependencyProjectID"
	
	destring dependencyprojectid, replace
	rename dependencyprojectid projectid
	
	// repoid already exists
	rename repoid from_repo_name
	destring from_repo_name, replace
	
	// match dependencyproject
// 	drop if projectid == "" | projectid == "runtime" | projectid == "]}" | projectid == "=>true" | projectid == "=>false}"
// 	destring projectid, replace
merge m:1 projectid using projectid_repoid.dta  // we lose 1,172,272 obs and match 4,518,057
	keep if _merge == 3
	drop _merge

	rename projectid to_project_name
	rename repoid to_repo_name
	sort from_repo_name to_repo_name
	order from_repo_name to_repo_name
save repo_match_dependencies_Pypi2.dta, replace
	keep to_repo_name from_repo_name
	duplicates drop
outsheet using "dependencies_Pypi-repo2.csv", delimiter(";") nonames replace
save "dependencies_Pypi-repo2.dta", replace // 1,023,801 obs
// RUN ./30_create_dependency_graph-1.6.0.py TO CREATE .GEXF FILE

// INTERLUDE: generate unique IDs for each repo, ensuring that they are the same for both to and from repos
use dependencies_Pypi-repo2.dta, clear
	rename from_repo_name repo_name 
	drop to_repo_name
save tmp_repo_id.dta, replace
use dependencies_Pypi-repo2.dta, clear
	rename to_repo_name repo_name 
	drop from_repo_name 
append using tmp_repo_id.dta
	duplicates drop
	egen id_repo = group(repo_name)
	sort id_repo
save id_repo_name.dta, replace

// MERGE newly generated unique to dependencies
use dependencies_Pypi-repo2.dta, clear
	rename from_repo_name repo_name
merge m:1 repo_name using id_repo_name.dta
	keep if _merge == 3
	drop _merge
	rename repo_name repo_from 
	rename id_repo id_repo_from 

	rename to_repo_name repo_name
merge m:1 repo_name using id_repo_name.dta
	keep if _merge == 3
	drop _merge
	rename repo_name repo_to
	rename id_repo id_repo_to
	
	sort id_repo_from id_repo_to
	order id_repo_from id_repo_to
save dependencies_Pypi-repo3.dta, replace	
outsheet using dependencies_Pypi-repo3.csv, delimiter(";") replace // 102,981 obs


//
// PREPARE CENTRALITIES -- ONCE GEPHI COMPUTES Average Degree, PageRank (0.85, 0.001), Eigenvector Centrality (Directed, 100 iterations)
//
insheet using gephi_analysis_dependencies_Pypi-repo2-lcc.csv, clear nonames
	drop v2 v3
	rename v1 repo_name 
	rename v4 in_degree
	rename v5 out_degree
	drop v6
	rename v7 pagerank
	rename v8 ev_centrality
	sort repo_name
save analysis_dependencies_Pypi-repo2-lcc.dta, replace


//
// START CREATING MASTER DATASET
//
cd ~/Dropbox/Papers/10_WorkInProgress/SoftwareNetworks/Data/Pypi
use repositories_Pypi.dta, clear
	drop description 
	rename repoid repo_name
	sort repo_name

merge 1:1 repo_name using analysis_dependencies_Pypi-repo2-lcc.dta
	keep if _merge == 3 // we lose 1,753,428 and keep 293,368 obs
	drop _merge

	rename size Size
	rename numcontributors NumContributors
	
	drop if Size == 0  // some packages have zero size
	rename numstars Popularity
	drop if Popularity > 50000 // alternative, cuts only the largest value and missing values
	rename numforks NumForks 
	rename numwatchers NumWatchers
save 10_popularity_centrality-repo.dta, replace

//
// INTERLUDE -- ANALYZE MASTER DATA
//
// 	winsor2 ev_centrality, replace cuts(0 95) trim
// 	winsor2 katz_centrality, replace cuts(0 95) trim
// 	winsor2 indeg_centrality, replace cuts(0,95) trim
// 	winsor2 in_degree, replace cuts(0,99) trim
// 	winsor2 out_degree, replace cuts(0,99) trim
// 	winsor2 NumContributors, replace cuts(0 99) trim
	
	// look at most central packages manually
	gsort - ev_centrality 
	
	// simple scatter plot
	scatter Popularity ev_centrality
	graph export Pypi_popularity-ev_centrality.jpg, replace
	
// 	scatter Popularity katz_centrality
// 	graph export Pypi_popularity-katz_centrality.jpg, replace

	scatter Popularity pagerank
	graph export Pypi_popularity-pagerank.jpg, replace
	
	label variable in_degree "in-degree"
	label variable out_degree "out-degree"
	scatter in_degree out_degree 
	graph export Pypi_indeg-outdeg.jpg, replace
	
	// binscatter
	winsor2 ev_centrality, replace cuts(0 95) trim
	binscatter Popularity ev_centrality
	graph export Pypi_bs_popularity-ev_centrality.jpg, replace
	
// 	binscatter Popularity katz_centrality
// 	graph export pypi_bs_popularity-katz_centrality.jpg, replace
	
	winsor2 pagerank, replace cuts(0 95) trim
	binscatter Popularity pagerank
	graph export pypi_bs_popularity-pagerank.jpg, replace
	
	// number of contributors vs popularity
	winsor2 Popularity, replace cuts(0 99) trim
	binscatter NumContributors Popularity
	graph export Pypi_bs_t99_numcontributors_popularity.jpg, replace
	
	binscatter NumContributors ev_centrality
	graph export Pypi_bs_t99_numcontributors_ev_centrality.jpg, replace

	binscatter NumContributors pagerank
	graph export Pypi_bs_t99_numcontributors_pagerank.jpg, replace

	// regressions
	regress Popularity NumContributors
	regress Popularity ev_centrality

save tmp_10_popularity_centrality-repo.dta, replace


//
// NOW CONTINUE TO CREATE MASTER DATASET FOR CATEGORICAL VARIABLES
//
use 10_popularity_centrality-repo.dta, clear
merge m:1 repo_name using id_repo_name.dta
	keep if _merge == 3
	drop _merge 
	
	local varlist Size Popularity NumForks NumWatchers NumContributors in_degree out_degree pagerank ev_centrality  // Add your variable names here

	foreach var in `varlist' {
		* Create quartiles using the "xtile" command
		xtile quartile = `var', nq(4)

		* Create four categorical variables based on quartiles for the current variable
		gen `var'_1 = (quartile == 1)
		gen `var'_2 = (quartile == 2)
		gen `var'_3 = (quartile == 3)
		gen `var'_4 = (quartile == 4)

		* Drop the temporary "quartile" variable if you don't need it
		drop quartile
	}
	order id_repo repo_name
save 20_master_Pypi.dta, replace
outsheet using 20_master_Pypi.csv, delimiter(";") replace


// ============================================================================
//
// Cran -- 1.6.0
//
// ============================================================================
cd ~/Dropbox/Papers/10_WorkInProgress/SoftwareNetworks/Data/Cran

//
// PREPARE FILES
//

// repositories
insheet using "repositories_Cran.csv", delimiter(";") names clear
	drop if repoid == "RepoID"
	destring size, replace
	destring numstars, replace
	destring numforks, replace
	destring numwatchers, replace
	destring numcontributors, replace

	destring repoid, replace
save "repositories_Cran.dta", replace

// projects
insheet using "projects_Cran.csv", delimiter(";") names clear
	drop if id == "ID"
	destring id, replace
	destring repoid, replace
save "projects_Cran.dta", replace
	rename id projectid
	keep projectid repoid
	
	drop if repoid == .
save projectid_repoid.dta, replace 

// REPO DEPENDENCIES 
//
// NB (from libraries.io): A repository dependency is a dependency upon a Version from a package manager has been specified in a manifest file, either as a manually added dependency committed by a user or listed as a generated dependency listed in a lockfile that has been automatically generated by a package manager and committed.
insheet using repo_dependencies_Cran.csv, names delimiter(";") clear
	drop id 
	drop projectname dependencyprojectname
	drop dependencyrequirements
	
	drop if dependencyprojectid == ""
	drop if dependencyprojectid == "DependencyProjectID"
	
	destring dependencyprojectid, replace
	rename dependencyprojectid projectid
	
	// repoid already exists
	rename repoid from_repo_name
	destring from_repo_name, replace
	
	// match dependencyproject
// 	drop if projectid == "" | projectid == "runtime" | projectid == "]}" | projectid == "=>true" | projectid == "=>false}"
// 	destring projectid, replace
merge m:1 projectid using projectid_repoid.dta // we lose 109,773 of 224,347 obs
	keep if _merge == 3
	drop _merge

	rename projectid to_project_name
	rename repoid to_repo_name
	sort from_repo_name to_repo_name
	order from_repo_name to_repo_name
save repo_match_dependencies_Cran2.dta, replace
	keep to_repo_name from_repo_name
	duplicates drop
outsheet using "dependencies_Cran-repo2.csv", delimiter(";") nonames replace
save "dependencies_Cran-repo2.dta", replace // 102,981 obs
// RUN ./30_create_dependency_graph-1.6.0.py TO CREATE .GEXF FILE

// INTERLUDE: generate unique IDs for each repo, ensuring that they are the same for both to and from repos
use dependencies_Cran-repo2.dta, clear
	rename from_repo_name repo_name 
	drop to_repo_name
save tmp_repo_id.dta, replace
use dependencies_Cran-repo2.dta, clear
	rename to_repo_name repo_name 
	drop from_repo_name 
append using tmp_repo_id.dta
	duplicates drop
	egen id_repo = group(repo_name)
	sort id_repo
save id_repo_name.dta, replace

// MERGE newly generated unique to dependencies
use dependencies_Cran-repo2.dta, clear
	rename from_repo_name repo_name
merge m:1 repo_name using id_repo_name.dta
	keep if _merge == 3
	drop _merge
	rename repo_name repo_from 
	rename id_repo id_repo_from 

	rename to_repo_name repo_name
merge m:1 repo_name using id_repo_name.dta
	keep if _merge == 3
	drop _merge
	rename repo_name repo_to
	rename id_repo id_repo_to
	
	sort id_repo_from id_repo_to
	order id_repo_from id_repo_to
save dependencies_Cran-repo3.dta, replace	
outsheet using dependencies_Cran-repo3.csv, delimiter(";") replace // 102,981 obs


//
// PREPARE CENTRALITIES -- ONCE GEPHI COMPUTES Average Degree, PageRank (0.85, 0.001), Eigenvector Centrality (Directed, 100 iterations)
//
insheet using gephi_analysis_dependencies_Cran-repo2-lcc.csv, clear nonames
	drop v2 v3
	rename v1 repo_name 
	rename v4 in_degree
	rename v5 out_degree
	drop v6
	rename v7 pagerank
	rename v8 ev_centrality
	sort repo_name
save analysis_dependencies_Cran-repo2-lcc.dta, replace


//
// START CREATING MASTER DATASET
//
cd ~/Dropbox/Papers/10_WorkInProgress/SoftwareNetworks/Data/Cran
use repositories_Cran.dta, clear
	drop description 
	rename repoid repo_name
	sort repo_name

merge 1:1 repo_name using analysis_dependencies_Cran-repo2-lcc.dta // we lose 145,051 of 172,217 obs
	keep if _merge == 3 
	drop _merge

	rename size Size
	rename numcontributors NumContributors
	
	drop if Size == 0  // some packages have zero size
	rename numstars Popularity
	drop if Popularity > 50000 // alternative, cuts only the largest value and missing values
	rename numforks NumForks 
	rename numwatchers NumWatchers
save 10_popularity_centrality-repo.dta, replace

//
// INTERLUDE -- ANALYZE MASTER DATA
//
// 	winsor2 ev_centrality, replace cuts(0 95) trim
// 	winsor2 katz_centrality, replace cuts(0 95) trim
// 	winsor2 indeg_centrality, replace cuts(0,95) trim
// 	winsor2 in_degree, replace cuts(0,99) trim
// 	winsor2 out_degree, replace cuts(0,99) trim
// 	winsor2 NumContributors, replace cuts(0 99) trim
	
	// look at most central packages manually
	gsort - ev_centrality 
	
	// simple scatter plot
	scatter Popularity ev_centrality
	graph export Cran_popularity-ev_centrality.jpg, replace
	
// 	scatter Popularity katz_centrality
// 	graph export Cran_popularity-katz_centrality.jpg, replace

	scatter Popularity pagerank
	graph export Cran_popularity-pagerank.jpg, replace
	
	label variable in_degree "in-degree"
	label variable out_degree "out-degree"
	scatter in_degree out_degree 
	graph export Cran_indeg-outdeg.jpg, replace
	
	// binscatter
	winsor2 ev_centrality, replace cuts(0 95) trim
	binscatter Popularity ev_centrality
	graph export Cran_bs_popularity-ev_centrality.jpg, replace
	
// 	binscatter Popularity katz_centrality
// 	graph export pypi_bs_popularity-katz_centrality.jpg, replace
	
	winsor2 pagerank, replace cuts(0 95) trim
	binscatter Popularity pagerank
	graph export pypi_bs_popularity-pagerank.jpg, replace
	
	// number of contributors vs popularity
	winsor2 Popularity, replace cuts(0 99) trim
	binscatter NumContributors Popularity
	graph export Cran_bs_t99_numcontributors_popularity.jpg, replace
	
	binscatter NumContributors ev_centrality
	graph export Cran_bs_t99_numcontributors_ev_centrality.jpg, replace

	binscatter NumContributors pagerank
	graph export Cran_bs_t99_numcontributors_pagerank.jpg, replace

	// regressions
	regress Popularity NumContributors
	regress Popularity ev_centrality

save tmp_10_popularity_centrality-repo.dta, replace


//
// NOW CONTINUE TO CREATE MASTER DATASET FOR CATEGORICAL VARIABLES
//
use 10_popularity_centrality-repo.dta, clear
merge m:1 repo_name using id_repo_name.dta
	keep if _merge == 3
	drop _merge 
	
	local varlist Size Popularity NumForks NumWatchers NumContributors in_degree out_degree pagerank ev_centrality  // Add your variable names here

	foreach var in `varlist' {
		* Create quartiles using the "xtile" command
		xtile quartile = `var', nq(4)

		* Create four categorical variables based on quartiles for the current variable
		gen `var'_1 = (quartile == 1)
		gen `var'_2 = (quartile == 2)
		gen `var'_3 = (quartile == 3)
		gen `var'_4 = (quartile == 4)

		* Drop the temporary "quartile" variable if you don't need it
		drop quartile
	}
	order id_repo repo_name
save 20_master_Cran.dta, replace
outsheet using 20_master_Cran.csv, delimiter(";") replace

	

// ============================================================================
//
// Cargo -- 1.4.0
//
// ============================================================================

//
// PRELIMINARY ANALYSES
//
cd ~/Dropbox/Papers/10_WorkInProgress/SoftwareProductionNetworks/Data/Cargo/
insheet using "dependencies_Cargo.csv", delimiter(";") names clear
	rename projectid id_from
	rename dependencyprojectid id_to
	keep id*
duplicates drop
	drop if id_from == "Project ID"
	destring id*, replace
outsheet using "dependencies_Cargo-projects.csv", delimiter(";") replace


//
// 1_maintainer_githubID.dta
//
cd ~/Dropbox/Papers/10_WorkInProgress/SoftwareProductionNetworks/Data/Cargo/covariates
insheet using "Maintainer_GithubID.csv", delimiter(",") names clear 
	rename project name_project
save "1_maintainer_githubID.dta", replace


//
// 2_maintainer_github_metadata.dta
//
cd ~/Dropbox/Papers/10_WorkInProgress/SoftwareProductionNetworks/Data/Cargo/covariates
insheet using "Maintainer_github_metadata.csv", delimiter(",") names clear 
	rename contributor_github_url maintainer_github_url

	// data not filled for 2/3 of the maintainers
	gen pct_code_review = round(100*code_review / contributions)
	gen pct_commits = round(100*commits / contributions)
	gen pct_issues = round(100*issues / contributions)
	gen pct_pull_requests = round(100*pull_requests / contributions)
	
save "2_maintainer_github_metadata-full.dta", replace
outsheet using "2_maintainer_github_metadata-full.csv", delimiter(",") replace

	sort maintainer_github_url year
	bysort maintainer_github_url: gen MaintainerSeniority = _N
	bysort maintainer_github_url: egen MaintainerActivity = sum(contributions)
	gen MaintainerAvgActivity = MaintainerActivity / MaintainerSeniority

	keep maintainer_github_url Maintainer*
	duplicates drop
	
save "2_maintainer_github_metadata.dta", replace
outsheet using "2_maintainer_github_metadata.csv", delimiter(",") replace
 

//
// 2_contributor_commits.dta
//
cd ~/Dropbox/Papers/10_WorkInProgress/SoftwareProductionNetworks/Data/Cargo/covariates
insheet using "Contributor_commits-clean.csv", delimiter(";") names clear 
	
	drop if contributor_github_url == ""  // 5,476 observations lost
	sort name_project contributor_github_url
	
save "2_contributor_commits-full.dta", replace
outsheet using "2_contributor_commits-full.csv", delimiter(",") replace

	bysort name_project: gen num_contributors_alt = _N

	bysort contributor_github_url: gen ContributorExperience = _N
	bysort contributor_github_url: egen ContributorTotalCommits = sum(contributor_commits)
	gen ContributorActivty = ContributorExperience / ContributorTotalCommits

	keep name_project contributor_github_url Contributor*
	
	duplicates drop
	
save "2_contributor_commits.dta", replace
outsheet using "2_contributor_commits.csv", delimiter(",") replace
	bysort name_project: egen TotalContributorCommits = sum(ContributorTotalCommits)
	keep name_project TotalContributorCommits
	duplicates drop 
save 12_contributors_project.dta, replace

//
// 3_covariates_maintainers.dta
//
// prepare maintainer data
use 2_maintainer_github_metadata-full.dta, clear
	bysort maintainer_github_url: egen total_contributions = sum(contributions)
	drop if total_contributions > 125000 // there seem to be a handful of either malicious or automated accounts that we are dropping here
	keep maintainer_github_url total_contributions
	duplicates drop
save 12_maintainer_contributions.dta, replace

// prepare contributor data 
insheet using Maintainer_GithubID.csv, delimiter(",") clear names
	merge m:1 maintainer_github_url using 12_maintainer_contributions.dta 
	drop if _merge != 3
	drop _merge
	rename project name_project
	bysort maintainer_github_url: gen num_repos_contributed_to = _N
	gen avg_contributions = total_contributions / num_repos_contributed_to
	duplicates drop
	sort name_project
	by name_project: egen tot_avg_maint_contrib = sum(avg_contributions)
save 12_maintainer_project.dta, replace // careful, total contributions are to all repositories a maintainer works on

	keep name_project tot_avg_maint_contrib
	duplicates drop
merge 1:1 name_project using 12_contributors_project.dta. // from before
	drop if _merge == 2
	drop _merge
	replace TotalContributorCommits = 0 if TotalContributorCommits == .
	rename tot_avg_maint_contrib TotalAverageMaintainerCommits
	gen TotalCommits = TotalAverageMaintainerCommits + TotalContributorCommits
save 12_project_commits.dta, replace

// prepare covariates
cd ~/Dropbox/Papers/10_WorkInProgress/SoftwareProductionNetworks/Data/Cargo/covariates
insheet using "covariates_maintainers-1.csv", delimiter(";") clear
	rename date_first_release str_date_first_release
	rename date_latest_release str_date_latest_release
	gen date_first_release = date(str_date_first_release, "YMD")
	format date_first_release %td
	gen date_latest_release = date(str_date_latest_release, "YMD")
	format date_latest_release %td
duplicates drop

	// Activity, Maturity
	gen Maturity = date_latest_release - date_first_release

	gen Activity = 30*num_total_releases / Maturity
	replace Activity = 0 if Activity == .

	// Other variables
	rename num_stars Popularity
	rename num_forks NumForks
	rename num_contributors NumContributors
	rename size_repository Size  // in Byte
	rename num_watchers NumWatchers 
	rename num_total_releases NumReleases
merge 1:1 name_project using 12_project_commits.dta 
	keep if _merge == 3
	drop _merge
	
	order name_project crates_url github_repo
outsheet using "3_covariates_maintainers.csv", delimiter(",") names replace
save "3_covariates_maintainers.dta", replace

	drop if crates_url == ""

	bysort crates_url: gen projects_per_crate = _N 
	bysort github_repo: gen projects_per_repo = _N 
	
	order github_repo projects_per_* name_project crates_url
	keep github_repo projects_per_* name_project crates_url
outsheet using "3_projects_per_repo.csv", delimiter(",") names replace
save "3_projects_per_repo.dta", replace
	
 
//
// 4_projects_cargo.dta -- to prepare mapping
//
cd ~/Dropbox/Papers/10_WorkInProgress/SoftwareProductionNetworks/Data/Cargo/
insheet using "projects_Cargo.csv", delimiter(";") clear
	rename projectid key1
	rename name name_project
	keep key1 name_project
duplicates drop
save "covariates/4_projects_cargo.dta", replace
outsheet using "covariates/4_projects_cargo.csv", delimiter(",") replace


// <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
//
// ALTERNATIVE dependency graph based on Cargo.csv obtained from 
// new scraper (2023-09-13)
//
cd ~/Dropbox/Papers/10_WorkInProgress/SoftwareProductionNetworks/Data/Cargo/
insheet using "dependencies_Cargo.csv", delimiter(";") names clear
	rename projectid id_from
	rename dependencyprojectid id_to
	keep id*
duplicates drop
	drop if id_from == "Project ID"
	destring id*, replace
	drop if id_from == . | id_to == .
outsheet using "dependencies_Cargo-projects2.csv", delimiter(";") replace
save dependencies_Cargo-projects2.dta, replace

cd ~/Dropbox/Papers/10_WorkInProgress/SoftwareProductionNetworks/Data/Cargo/
insheet using "projects_Cargo.csv", delimiter(";") names clear
	rename name project_name
	rename projectid project_id 
save projects_Pypi.dta, replace
	keep project_name project_id
	duplicates drop
save project_id_name.dta, replace

insheet using "Cargo.csv", delimiter(";") names clear
	sort github_repo project package_manager_url
	order github_repo project package_manager_url
	drop if github_repo == ""
	
	rename project project_name
merge m:1 project_name using project_id_name.dta 
	keep if _merge == 3
	drop _merge
	
	keep github_repo project_id 
	duplicates drop
	sort github_repo project_id
	
	bysort project_id : gen foo = _N
	drop if foo > 1
	drop foo
save github_projectid.dta, replace

//
// ALTERNATIVE: repo-based dependency graph
//
use dependencies_Cargo-projects2.dta, clear
	rename id_from project_id 
merge m:1 project_id using github_projectid.dta
	keep if _merge == 3
	drop _merge 
	rename project_id id_from 
	rename github_repo repo_from 
	
	rename id_to project_id 
merge m:1 project_id using github_projectid.dta
	keep if _merge == 3
	drop _merge 
	rename project_id id_to 
	rename github_repo repo_to 
	
	keep repo_from repo_to

	duplicates drop
save repo_dependencies2.dta, replace
// >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>


//
// dependencies_Cargo-repo -- create repo-based dependency graph
//
cd ~/Dropbox/Papers/10_WorkInProgress/SoftwareProductionNetworks/Data/Cargo/
insheet using "dependencies_Cargo.csv", delimiter(";") names clear
	rename projectid from
	rename dependencyprojectid to
	keep from to
	duplicates drop
	drop if from == "Project ID"
	
	// match project names
	rename from key1
	destring key1, replace
merge m:1 key1 using covariates/4_projects_cargo.dta
	keep if _merge == 3
	drop _merge
	rename key1 id_from 
	// now match repos to project names
merge m:1 name_project using covariates/3_projects_per_repo.dta 
	keep if _merge == 3
	drop _merge
	rename name_project name_from
	rename github_repo repo_from
	rename crates_url crates_from
	drop projects*
	
	// match project names
	rename to key1
	destring key1, replace
merge m:1 key1 using covariates/4_projects_cargo.dta
	keep if _merge == 3
	drop _merge
	rename key1 id_to 
	// now match repos to project names
merge m:1 name_project using covariates/3_projects_per_repo.dta 
	keep if _merge == 3
	drop _merge
	rename name_project name_to
	rename github_repo repo_to
	rename crates_url crates_to
	drop projects*
save dependencies_Cargo-repo-tmp.dta, replace

use dependencies_Cargo-repo-tmp.dta, clear
	keep repo_from
	duplicates drop
	rename repo_from repo_name
save repo_from.dta, replace

use dependencies_Cargo-repo-tmp.dta, clear
	keep repo_to
	rename repo_to repo_name
	duplicates drop
append using repo_from.dta	
	duplicates drop
	
	sort repo_name
	egen id_repo = group(repo_name)
	replace id_repo = 0 if id_repo == .
save id_repo_name.dta, replace

	// generate repo ids
use dependencies_Cargo-repo-tmp.dta, clear
	rename repo_from repo_name
merge m:1 repo_name using id_repo_name.dta
	keep if _merge == 3
	drop _merge
	rename repo_name repo_from 
	rename id_repo id_repo_from 

	rename repo_to repo_name
merge m:1 repo_name using id_repo_name.dta
	keep if _merge == 3
	drop _merge
	rename repo_name repo_to
	rename id_repo id_repo_to
	
	gen _missing = 0
	replace _missing = 1 if repo_from == "" | repo_to == ""
	replace _missing = 2 if repo_from == "" & repo_to == ""
	
	order id_from id_repo_from id_to id_repo_to _missing name_from name_to repo_from repo_to crates_from crates_to
	sort name_from name_to
save dependencies_Cargo-repo.dta, replace	
outsheet using dependencies_Cargo-repo.csv, delimiter(";") replace
	
	// now save only those for which we have to and from repos
	keep if _missing == 0
outsheet using dependencies_Cargo-repo-nomissing.csv, nonames delimiter(";") replace
	keep id_repo_from id_repo_to // repo_from repo_to
	duplicates drop
	sort id_repo_from id_repo_to
outsheet using dependencies_Cargo-repo-nomissing.dat, nonames delimiter(" ") replace


//
// MAIN ANALYSIS -- REPO LEVEL
//
// PREPARE CENTRALITIES
cd ~/Dropbox/Papers/10_WorkInProgress/SoftwareProductionNetworks/Data/Cargo/
insheet using analysis_dependencies_Cargo-repo-nomissing.csv, delimiter(";") names clear
	rename id_node id_repo
save 6_centralities_Cargo-repo-nonmissing.dta, replace

// CREATE MASTER DATASET
cd ~/Dropbox/Papers/10_WorkInProgress/SoftwareProductionNetworks/Data/Cargo/covariates
use 3_covariates_maintainers.dta, clear
	keep name_project NumForks NumContributors NumReleases Size Popularity NumWatchers Total*

merge 1:1 name_project using 3_projects_per_repo.dta
	drop if _merge != 3
	drop _merge
	
	drop if github_repo == ""
	
	bysort github_repo: egen foo = total(NumReleases)
	drop NumReleases
	rename foo NumReleases
	
	order github_repo NumReleases Size NumContributors NumForks Popularity NumWatchers
	keep github_repo NumReleases Size NumContributors NumForks Popularity NumWatchers Total*
	duplicates drop

	// there are some edge cases here that we are alleviating this way; the issue is in the raw data
	foreach var of varlist NumReleases-TotalCommits {
		bysort github_repo: egen foo = mean(`var')
		drop `var'
		rename foo `var'
	}
	duplicates drop
	
	rename github_repo repo_name
merge 1:1 repo_name using ../id_repo_name.dta
	keep if _merge == 3
	drop _merge
	rename repo_name github_repo
	
merge 1:1 id_repo using ../6_centralities_Cargo-repo-nonmissing.dta
	keep if _merge == 3
	drop _merge 
save ../10_popularity_centrality-projects.dta, replace

use ../10_popularity_centrality-projects.dta, clear
	drop if Size == 0  // some packages have zero size

	// truncate Popularity and centrality 
	
	// simple scatter plot
	scatter Popularity ev_centrality
	graph export popularity-ev_centrality.jpg, replace
	
	scatter Popularity katz_centrality
	graph export popularity-katz_centrality.jpg, replace

	scatter Popularity indeg_centrality
	graph export popularity-indeg_centrality.jpg, replace
	
	// binscatter
// 	winsor2 ev_centrality, replace cuts(0 99) trim
	binscatter Popularity ev_centrality
	graph export bs_popularity-ev_centrality.jpg, replace
	
// 	winsor2 katz_centrality, replace cuts(0 99) trim
	binscatter Popularity katz_centrality
	graph export bs_popularity-katz_centrality.jpg, replace
	
// 	winsor2 indeg_centrality, replace cuts(0 99) trim
	binscatter Popularity indeg_centrality
	graph export bs_popularity-indeg_centrality.jpg, replace

	// number of releases and popularity should be truncated as well
	winsor2 NumReleases, replace cuts(0 99) trim
	winsor2 Popularity, replace cuts(0 99) trim
	
	// number of contributors vs popularity
	binscatter NumContributors Popularity
	graph export bs_t99_numcontributors_popularity.jpg, replace
	
	binscatter NumContributors katz_centrality
	graph export bs_t99_numcontributors_katz_centrality.jpg, replace

	binscatter NumContributors indeg_centrality
	graph export bs_t99_numcontributors_indeg_centrality.jpg, replace

	// regressions
	regress Popularity NumContributors
	regress Popularity katz_centrality
	
	// 
	// activity vs. popularity plots
	//
	gen top_pop = 0
	replace top_pop = 1 if Popularity > 1800 // 138 changes, 99pct
	gen bot_pop = 0 
	replace bot_pop = 1 if Popularity <= 4 // 3581 changes, 50pct
	
	gen top_katz_cent = 0
	replace top_katz_cent = 1 if katz_centrality > 0.024288 // 77 changes, 99pct
	gen bot_katz_cent = 0
	replace bot_katz_cent = 1 if katz_centrality <= 0.0011881 // 4036 changes, 50pct
	
	gen top_indeg_cent = 0
	replace top_indeg_cent = 1 if indeg_centrality > 0.0068458 // 69 changes, 99pct
	gen bot_indeg_cent = 0
	replace bot_indeg_cent = 1 if indeg_centrality <= 0 // 4036 changes, 50 pct

save ../tmp_10_popularity_centrality-projects.dta, replace


//
// CENTRALTIES
//
// KATZ CENTRALITY VS. POPULARITY
cd ~/Dropbox/Papers/10_WorkInProgress/SoftwareProductionNetworks/Data/Cargo/covariates
use ../tmp_10_popularity_centrality-projects.dta, clear
	
	keep if top_katz_cent == 1 | top_pop == 1. // 331 obs left
	gen foo = log(Size / (1024*1024))
	drop Size
	rename foo logSize // MB
	
	hist logSize if top_katz_cent == 1
	hist logSize if top_pop == 1
	
	hist NumContributors if top_katz_cent == 1
	hist NumContributors if top_pop == 1
	
	twoway (hist logSize if top_katz_cent == 1, start(-6) width(2) color(red%30)) ///
		(hist logSize if top_pop == 1, start(-5) width(2) color(blue%30)), ///
		legend(order(1 "Most Central" 2 "Most Popular"))
	graph export hist_logSize_CentralPopular.jpg, replace
		
	twoway (hist NumContributors if top_katz_cent == 1, start(0) width(125) color(red%30)) ///
		(hist  NumContributors if top_pop == 1, start(0) width(125) color(blue%30)), ///
		legend(order(1 "Most Central" 2 "Most Popular"))
	graph export hist_NumContributors_CentralPopular.jpg, replace
		
	twoway (hist NumReleases if top_katz_cent == 1, start(0) width(10) color(red%30)) ///
		(hist   NumReleases if top_pop == 1, start(0) width(10) color(blue%30)), ///
		legend(order(1 "Most Central" 2 "Most Popular"))
	graph export hist_NumReleases_CentralPopular.jpg, replace
save ../11_top_popularity_centrality-projects-1.dta, replace

// TOP VS. BOTTOM KATZ CENTRALITY
cd ~/Dropbox/Papers/10_WorkInProgress/SoftwareProductionNetworks/Data/Cargo/covariates
use ../tmp_10_popularity_centrality-projects.dta, clear
	
	keep if top_katz_cent == 1 | bot_katz == 1
	gen foo = log(Size / (1024*1024))
	drop Size
	rename foo logSize // MB
	
	hist logSize if top_katz_cent == 1
	hist logSize if bot_katz_cent == 1
	
	hist NumContributors if top_katz_cent == 1
	hist NumContributors if bot_katz_cent == 1
	
	twoway (hist logSize if top_katz_cent == 1, start(-8) width(2) color(red%30)) ///
		(hist logSize if bot_katz_cent == 1, start(-8) width(2) color(blue%30)), ///
		legend(order(1 "Log(Size) Most Central" 2 "Log(Size) Least Central"))
	graph export hist_logSize_TopLeast_Central.jpg, replace
		
	twoway (hist NumContributors if top_katz_cent == 1, start(0) width(100) color(red%30)) ///
		(hist  NumContributors if bot_katz_cent == 1, start(0) width(100) color(blue%30)), ///
		legend(order(1 "NumContrib Most Central" 2 "NumContrib Least Central"))
	graph export hist_NumContributors_TopLeast_Central.jpg, replace
		
	twoway (hist NumReleases if top_katz_cent == 1, start(0) width(10) color(red%30)) ///
		(hist   NumReleases if bot_katz_cent == 1, start(0) width(10) color(blue%30)), ///
		legend(order(1 "NumReleases Most Central" 2 "NumReleases Least Central"))
	graph export hist_NumReleases_TopLeast_Central.jpg, replace 
save ../11_top_popularity_centrality-projects-2.dta, replace

// TOP VS. BOTTOM POPULARITY
cd ~/Dropbox/Papers/10_WorkInProgress/SoftwareProductionNetworks/Data/Cargo/covariates
use ../tmp_10_popularity_centrality-projects.dta, clear
	
	keep if top_pop == 1 | bot_pop == 1
	gen foo = log(Size / (1024*1024))
	drop Size
	rename foo logSize // MB
	
	hist logSize if top_pop == 1
	hist logSize if bot_pop == 1
	
	hist NumContributors if top_pop == 1
	hist NumContributors if bot_pop == 1
	
	twoway (hist logSize if top_pop == 1, start(-8) width(2) color(red%30)) ///
		(hist logSize if bot_pop == 1, start(-8) width(2) color(blue%30)), ///
		legend(order(1 "Log(Size) Most Popular" 2 "Log(Size) Least Popular"))
	graph export hist_logSize_TopLeast_Popularity.jpg, replace
		
	twoway (hist NumContributors if top_pop == 1, start(0) width(100) color(red%30)) ///
		(hist  NumContributors if bot_pop == 1, start(0) width(100) color(blue%30)), ///
		legend(order(1 "NumContrib Most Popular" 2 "NumContrib Least Popular"))
	graph export hist_NumContributors_TopLeast_Popularity.jpg, replace
		
	twoway (hist NumReleases if top_pop == 1, start(0) width(10) color(red%30)) ///
		(hist   NumReleases if bot_pop == 1, start(0) width(10) color(blue%30)), ///
		legend(order(1 "NumReleases Most Popular" 2 "NumReleases Least Popular"))
	graph export hist_NumReleases_TopLeast_Popular.jpg, replace 
save ../11_top_popularity_centrality-projects-3.dta, replace

// INDEGREE CENTRALITY VS. POPULARITY
cd ~/Dropbox/Papers/10_WorkInProgress/SoftwareProductionNetworks/Data/Cargo/covariates
use ../tmp_10_popularity_centrality-projects.dta, clear	
	keep if top_indeg_cent == 1 | top_pop == 1. // 331 obs left
	gen foo = log(Size / (1024*1024))
	drop Size
	rename foo logSize // MB
	
	hist logSize if top_indeg_cent == 1
	hist logSize if top_pop == 1
	
	hist NumContributors if top_indeg_cent == 1
	hist NumContributors if top_pop == 1

	twoway (hist logSize if top_indeg_cent == 1, start(-6) width(2) color(red%30)) ///
		(hist logSize if top_pop == 1, start(-5) width(2) color(blue%30)), ///
		legend(order(1 "Most Central" 2 "Most Popular"))
	graph export hist_logSize_IndegCentralPopular.jpg, replace
		
	twoway (hist NumContributors if top_indeg_cent == 1, start(0) width(125) color(red%30)) ///
		(hist  NumContributors if top_pop == 1, start(0) width(125) color(blue%30)), ///
		legend(order(1 "Most Central" 2 "Most Popular"))
	graph export hist_NumContributors_IndegCentralPopular.jpg, replace
		
	twoway (hist NumReleases if top_indeg_cent == 1, start(0) width(10) color(red%30)) ///
		(hist   NumReleases if top_pop == 1, start(0) width(10) color(blue%30)), ///
		legend(order(1 "Most Central" 2 "Most Popular"))
	graph export hist_NumReleases_IndegCentralPopular.jpg, replace
save ../11_top_popularity_centrality-projects-4.dta, replace


//
// NUMCONTRIBUTORS VS. SIZE
//
cd ~/Dropbox/Papers/10_WorkInProgress/SoftwareProductionNetworks/Data/Cargo
use tmp_10_popularity_centrality-projects.dta, clear
// 	gen foo = log(Size / (1024*1024))
// 	drop Size
// 	rename foo logSize // MB
	gen foo = Size/(1024*1024)
	drop Size
	rename foo Size
	
// 	winsor2 Size, replace cuts(0,90) trim
// 	winsor2 TotalCommits, replace cuts(0,90) trim
// 	winsor2 NumContributors, replace cuts(0,90) trim. // TODO: careful
	drop if TotalCommits > 1000000  // all of these are somewhat suspicious repositories
	drop if NumContributors > 150
	drop if Size > 38.3  // p99
	
	scatter NumContributors Size
	graph export NumContrib_Size.jpg, replace
	
	scatter TotalCommits Size
	graph export TotalCommits_Size.jpg, replace
	
	scatter TotalCommits NumContributors
	graph export TotalCommits_NumContributors.jpg, replace

	gen Binned_NumContrib = round(NumContributors/10,1)
	gen AvgNumCommits = TotalCommits / NumContributors
	bysort Binned_NumContrib: egen AvgCommits = mean(AvgNumCommits)
	keep Binned_NumContrib AvgCommits
	duplicates drop
	
	line AvgCommits Binned_NumContrib
	graph export AvgCommits_BinnedNumContrib.jpg, replace
		
	

// ==============================================================================
//
// NOT USED
//
// ==============================================================================
