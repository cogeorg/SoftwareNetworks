{"cells":[{"cell_type":"markdown","metadata":{"id":"YdFl3x2SBBFf"},"source":["#Package Section"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"1feGDRiBv-2-"},"outputs":[],"source":["import sys\n","import numpy as np\n","import copy\n","from numpy import linalg as LA\n","from tensorflow import keras\n","from tensorflow.keras.utils import to_categorical\n","from sklearn.metrics.cluster import adjusted_rand_score\n","from sklearn.cluster import KMeans\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n","from sklearn import metrics\n","import time\n","# node2vec\n","# from node2vec import Node2Vec\n","import networkx as nx\n","# for sparse matrix\n","from scipy import sparse\n","#early stop\n","from keras.callbacks import EarlyStopping\n","from tensorflow.keras.callbacks import ModelCheckpoint"]},{"cell_type":"markdown","metadata":{"id":"4N9L480srX8a"},"source":["#Classes and functions"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"QpkZjKLAskam"},"outputs":[],"source":["# Supress/hide the warning\n","# invalide devide resutls will be nan\n","np.seterr(divide='ignore', invalid='ignore')\n","\n","############------------Auto_select_method_start-----------------###############\n","def Run(case, learn_opt, **kwargs):\n","  \"\"\"\n","    input X can be a list of one of these format below:\n","    1. python list of n*n adjacency matrices.\n","    2. python list of s*2 edge lists. \n","    3. python list of s*3 edge lists. \n","    input Y can be these choices below:\n","    1. no Y input. The default will be [2,3,4,5] -- K range for clusters.\n","    2. n*1 class -- label vector. Positive labels are knwon labels and -1 indicate unknown labels.\n","    3. A range of potential number of clusters -- K (K clusters in total), i.e., [3, 4, 5].\n","    \n","    if input X is n*n adjacency =>  s*3 edg list\n","    if input X is s*2 => s*3 edg list\n","    \n","    Vertex size should be >10.\n","\n","    Clustering / Classification\n","    The program automaticlly decide to run clustering or classification.\n","    1. If Y is a given cluster range, do clustering (case 1,3 for Y).\n","    2. If Y is a label vector (case 2 for Y), do classification.\n","    For classification: semi-supervised learning, supervised learning methods. \n","                        see the \"Learner\" defined below. \n","\n","    \n","    Supervised learning \"Learner\":\n","      **Note the input trining set (X) need has fully known labels in Y.\n","      Learner = 1 run LDA, test on test set\n","      Learner = 0 run NN, test on test set\n","\n","    Semi-supervised learning \"Learner\": \n","       **Note the input trining set (X) need some unknown label(s) in Y.     \n","      Learner=0 means embedding via known label, do not learn the unknown labels. \n","           Since only some nodes in the training set has known label, \n","           the test set is the unknwon labeled set, which is compared with \n","           the original labels of the unknown set\n","      Learner=1 means embedding via partial label, then learn unknown label via LDA.\n","        this runs semi-supervised learning with NN, \n","        the test will be on the result labels with the original labels\n","\n","      Learner=2 means embedding via partial label, then learn unknown label via two-layer NN.\n","        this runs semi-supervised learning with NN, \n","        the test will be on the result labels with the original labels\n","    \n","\n","  \"\"\"\n","  defaultKwargs = {'Y':[2,3,4,5], 'DiagA': True,'Correlation': True,'Laplacian': False,\n","                  'Learner': 1, 'LearnerIter': 0, 'MaxIter': 50, 'MaxIterK': 5,\n","                  'Replicates': 3, 'Attributes': False, 'neuron': 20, 'activation': 'relu',\n","                   'emb_opt': 'AEE', 'sparse_opt': 'None', 'Batch_input': False} \n","  kwargs = { **defaultKwargs, **kwargs }\n","  train_time = \"no seperate training time for semisuperviised learning yet\"\n","  total_begin = time.time()\n","  eval = Evaluation()\n","  kwargs_for_DataPreprocess =  {k: kwargs[k] for k in ['DiagA', 'Laplacian', 'Correlation', 'Attributes', 'emb_opt']}\n","  Dataset = DataPreprocess(case, **kwargs_for_DataPreprocess)\n","  \n","  Y = case.Y\n","  n = case.n\n","\n","  # auto check block\n","  # if the option is not clustering, but the Y does not contain labels (known/unknwon) for n nodes. \n","  if (learn_opt != \"c\") and (len(Y) != n):\n","    learn_opt = \"c\" # do clustering\n","    print(\"The given Y do not have the same size as the node.Y is assumed as cluster number range.\",\n","    \"Clustering will be performed.\",\n","    \"If you want to do classification, stop the current run, reimport the Y with the right format then run again.\",\n","    sep = \"\\n\")\n"," \n","  # clustering\n","  if learn_opt == 'c':  \n","    cluster = Clustering(Dataset)\n","    Z, Y, W, meanSS = cluster.cluster_main()\n","    ari = eval.clustering_test(Y, case.Y_ori)\n","    print(\"ARI: \", ari)\n","\n","  # supervised learning\n","  if learn_opt == \"su\":\n","    Dataset = Dataset.supervise_preprocess()\n","    kwargs_for_learner = {k: kwargs[k] for k in ['Learner', 'LearnerIter', 'Batch_input']}\n","    train_strat = time.time()\n","    if kwargs['Learner'] == 1:\n","      lda = LDA(Dataset, **kwargs_for_learner)\n","      lda_res = lda.LDA_Learner(lda.DataSets)\n","      acc = eval.LDA_supervise_test(lda_res, Dataset.z_test, Dataset.Y_test)\n","    if kwargs['Learner'] == 0:\n","      gnn = GNN(Dataset, **kwargs_for_learner)\n","      gnn_res = gnn.GNN_complete()\n","      acc = eval.GNN_supervise_test(gnn_res, Dataset.z_test, Dataset.Y_test)\n","    train_end = time.time()\n","    train_time = train_end - train_strat \n","    print(\"acc: \", acc)\n","  \n","  # semisupervised learning\n","  if learn_opt == \"se\":\n","    Dataset = Dataset.semi_supervise_preprocess()\n","    kwargs_for_learner = {k: kwargs[k] for k in ['Learner', 'LearnerIter', 'Batch_input']}\n","    if kwargs['Learner'] == 2:\n","      gnn = GNN(Dataset, **kwargs_for_learner)\n","      gnn_res = gnn.GNN_complete()\n","      acc = eval.GNN_semi_supervised_learn_test(gnn_res.Y, case.Y_ori)      \n","    if kwargs['Learner'] == 1:\n","      lda = LDA(Dataset, **kwargs_for_learner)\n","      lda_res = lda.LDA_Iter()\n","      acc = eval.GNN_semi_supervised_learn_test(lda_res.Y, case.Y_ori)\n","    if kwargs['Learner'] == 0:\n","      gnn = GNN(Dataset, **kwargs_for_learner)\n","      gnn_res = gnn.GNN_complete()      \n","      acc = eval.GNN_semi_supervised_not_learn_test(gnn_res, Dataset, case)\n","    print(\"acc: \", acc)\n","  \n","  total_end = time.time()\n","  emb_time = Dataset.embed_time\n","  total_time = total_end - total_begin\n","  print(\"--- embed %s seconds ---\" % emb_time)\n","  print(\"--- train %s seconds ---\" % train_time)\n","  print(\"--- total %s seconds ---\" % total_time)\n","\n","  Z_ori = Dataset.Z\n","  W_ori = Dataset.W\n","\n","  sparse_opt = kwargs['sparse_opt']\n","  Z = To_single_sparse_matrix(Dataset.Z, sparse_opt)\n","  W = To_multi_sparse_matrix(Dataset.W, sparse_opt)\n","\n","  return acc, train_time, emb_time, total_time, Z, W, Z_ori, W_ori\n"," \n","############------------node2vec_embed_start--------------------################\n","def node2vec_embed(X):\n","  G = nx.from_numpy_matrix(X)\n","  # use default setting from https://github.com/eliorc/node2vec\n","  node2vec = Node2Vec(G, dimensions=64, walk_length=30, num_walks=200, workers=4)\n","  # Embed nodes, use default setting from https://github.com/eliorc/node2vec\n","  model = node2vec.fit(window=2, min_count=1, batch_words=4)\n","  # get embedding matrix\n","  Z = model.wv.vectors\n","  return Z\n","\n","############------------node2vec_embed_end----------------------################\n","############------------graph_encoder_embed_start----------------###############\n","def graph_encoder_embed(X,Y,n,**kwargs):\n","  \"\"\"\n","    input X is s*3 edg list: nodei, nodej, connection weight(i,j)\n","    graph embedding function\n","  \"\"\"\n","  defaultKwargs = {'Correlation': True}\n","  kwargs = { **defaultKwargs, **kwargs}\n","\n","  #If Y has more than one dimention , Y is the range of cluster size for a vertex. e.g. [2,10], [2,5,6]\n","  # check if Y is the possibility version. e.g.Y: n*k each row list the possibility for each class[0.9, 0.1, 0, ......]\n","  possibility_detected = False\n","  if Y.shape[1] > 1:\n","    k = Y.shape[1]\n","    possibility_detected = True\n","  else:\n","    # assign k to the max along the first column\n","    # Note for python, label Y starts from 0. Python index starts from 0. thus size k should be max + 1\n","    k = Y[:,0].max() + 1\n","\n","  #nk: 1*n array, contains the number of observations in each class\n","  #W: encoder marix. W[i,k] = {1/nk if Yi==k, otherwise 0}\n","  nk = np.zeros((1,k))\n","  W = np.zeros((n,k))\n","\n","  if possibility_detected:\n","    # sum Y (each row of Y is a vector of posibility for each class), then do element divid nk.\n","    nk=np.sum(Y, axis=0)\n","    W=Y/nk\n","  else:\n","    for i in range(k):\n","      nk[0,i] = np.count_nonzero(Y[:,0]==i)\n","\n","    for i in range(Y.shape[0]):\n","      k_i = Y[i,0]\n","      if k_i >=0:\n","        W[i,k_i] = 1/nk[0,k_i]\n","  \n","\n","  # Edge List Version in O(s)\n","  Z = np.zeros((n,k))\n","  i = 0\n","  for row in X:\n","    [v_i, v_j, edg_i_j] = row\n","    v_i = int(v_i)\n","    v_j = int(v_j)\n","    if possibility_detected:\n","      for label_j in range(k):\n","        Z[v_i, label_j] = Z[v_i, label_j] + W[v_j, label_j]*edg_i_j\n","        if v_i != v_j:\n","          Z[v_j, label_j] = Z[v_j, label_j] + W[v_i, label_j]*edg_i_j\n","    else:\n","      label_i = Y[v_i][0] \n","      label_j = Y[v_j][0]\n","\n","      if label_j >= 0:\n","        Z[v_i, label_j] = Z[v_i, label_j] + W[v_j, label_j]*edg_i_j\n","      if (label_i >= 0) and (v_i != v_j):\n","        Z[v_j, label_i] = Z[v_j, label_i] + W[v_i, label_i]*edg_i_j\n","  \n","  # Calculate each row's 2-norm (Euclidean distance). \n","  # e.g.row_x: [ele_i,ele_j,ele_k]. norm2 = sqr(sum(ele_i^2+ele_i^2+ele_i^2))\n","  # then divide each element by their row norm\n","  # e.g. [ele_i/norm2,ele_j/norm2,ele_k/norm2]\n","  if kwargs['Correlation']:\n","    row_norm = LA.norm(Z, axis = 1)\n","    reshape_row_norm = np.reshape(row_norm, (n,1))\n","    Z = np.nan_to_num(Z/reshape_row_norm)\n","  \n","  return Z, W\n","\n","\n","def multi_graph_encoder_embed(DataSets, Y, **kwargs):\n","  \"\"\"\n","    input X contains a list of s3 edge list\n","    get Z and W by using graph emcode embedding\n","    Z is the concatenated embedding matrix from multiple graphs\n","    if there are attirbutes provided, add attributes to Z\n","    W is a list of weight matrix Wi\n","  \"\"\"\n","  kwargs_single = {**kwargs}\n","\n","  X = DataSets.X\n","  n = DataSets.n\n","  U = DataSets.U\n","  Graph_count = DataSets.Graph_count\n","  attributes = DataSets.attributes\n","  kwargs = DataSets.kwargs\n","\n","  W = []\n","\n","  for i in range(Graph_count):\n","    if i == 0:\n","      [Z, Wi] = graph_encoder_embed(X[i],Y,n,**kwargs_single)\n","    else:\n","      [Z_new, Wi] = graph_encoder_embed(X[i],Y,n,**kwargs)\n","      Z = np.concatenate((Z, Z_new), axis=1)\n","    W.append(Wi)\n","\n","  # if there is attributes matrix U provided, add U\n","  if attributes:\n","    # add U to Z side by side\n","    Z = np.concatenate((Z, U), axis=1)\n","\n","  return Z, W\n","\n","############------------graph_encoder_embed_end------------------###############\n","\n","############------------DataPreprocess_start---------------------###############\n","class DataPreprocess:\n","  def __init__(self, Dataset_input, **kwargs):\n","    self.kwargs = self.kwargs_construct(**kwargs)\n","    # Note, since every element in multi-graph list X has the same size and \n","    # node index, there will be only one column in Y for the node labels\n","    self.Y = Dataset_input.Y  \n","    self.n = Dataset_input.n\n","    (self.X, self.Graph_count, self.embed_time) = self.input_prep(Dataset_input.X)    \n","    (self.attributes, self.U) = self.check_attributes()\n","    self.Dataset_input = Dataset_input\n","\n","\n","  def kwargs_construct(self, **kwargs):\n","    defaultKwargs = {'DiagA': True,'Laplacian': False,  #input_prep\n","                     'Correlation': True,      # graph_encoder_embed\n","                     'Attributes': False,      # GNN_preprocess\n","                     }\n","    kwargs = { **defaultKwargs, **kwargs}  # update the args using input_args\n","    return kwargs      \n","\n","\n","  def check_attributes(self):\n","    \"\"\"\n","      return attributes detected flag and attributes U\n","    \"\"\"\n","    kwargs = self.kwargs\n","    \n","    Attributes_detected = False\n","    U = None\n","\n","    if kwargs[\"Attributes\"]:\n","      U = kwargs[\"Attributes\"]\n","      if U.shape[0] == n:\n","        Attributes_detected = True\n","      else:\n","        print(\"Attributes need to have the same size as the nodes.\\\n","        If n nodes, need n rows\")    \n","    return Attributes_detected, U\n","\n","  \n","  def test_edg_list_to_adj(self, n_test, n, edg_list):\n","    adj = np.zeros((n_test,n))\n","\n","    for row in edg_list:\n","      [node_i, node_j, edge_i_j] = row\n","      adj[node_i, node_j] = edge_i_j\n","    \n","    return adj\n","\n","\n","  def input_prep(self, X):\n","    kwargs = self.kwargs\n","    # if X is a single numpy object, put this numpy object in a list\n","    if type(X) == np.ndarray:\n","      X = [X]\n","    \n","    ## Now X is a list of numpy objects\n","    # each element can be a numpy object for adjacency matrix or edge list\n","    Graph_count = len(X)  \n","\n","    # AEE needs X to be a list of edge list\n","    if kwargs[\"emb_opt\"] == \"AEE\":\n","      X, embed_time = self.input_prep_AEE(X, Graph_count)\n","    # Node2Vec only needs a list of adjacency matrix \n","    if kwargs[\"emb_opt\"] == \"Node2Vec\":\n","      embed_time = 0\n","      pass\n","\n","    return X, Graph_count, embed_time\n","\n","  \n","  def input_prep_AEE(self, X, Graph_count):\n","    \"\"\"\n","      X may be a single numpy object or a list of numpy objects\n","      The multi-graph input X is assumed has the same node numbers \n","      for each element in X, and the node are indexed the same way \n","      amonge the elements. e.g. node_0 in X[1] is the same node_0 in X[2]. \n","      return X as a list of s*3 edge lists\n","      return n, which is the total number of nodes\n","    \"\"\"\n","\n","    # need total labeled number n \n","    # if try to get from the edg list, it may miss the node that has no connection with others but has label \n","    n = self.n\n","\n","    embed_time = 0 \n","    for i in range(Graph_count):\n","      X_tmp = X[i]\n","      X_tmp = self.to_s3_list(X_tmp) \n","     \n","      # count the time for laplacian and diagnal only\n","      embed_begin = time.time() \n","      X_tmp = self.single_X_prep(X_tmp, n)\n","      embed_end = time.time()\n","      embed_time += (embed_end - embed_begin)\n","      \n","      X[i] = X_tmp\n","    \n","    return X, embed_time\n","\n","\n","  def to_s3_list(self,X):\n","    \"\"\"\n","      the input X is a signle graph, can be adjacency matrix or edgelist\n","      this function will return a s3 edge list\n","    \"\"\"\n","    (s,t) = X.shape\n","\n","    if s == t:\n","      # convert adjacency matrix to edgelist\n","      X = self.adj_to_edg(X);\n","    else:\n","      # for either s*2 or s*3 case, calculate n -- vertex number\n","      if t == 2: \n","        # enlarge the edgelist to s*3 by adding 1 to the thrid position as adj(i,j)\n","        X = np.insert(X, 1, np.ones(s,1))\n","\n","    return X\n","\n","\n","  def single_X_prep(self, X, n):\n","    \"\"\"\n","      input X is a single S3 edge list\n","      this adds Diagnal augement and Laplacian normalization to the edge list\n","    \"\"\"\n","    kwargs = self.kwargs\n","\n","    X = X.astype(np.float32)\n","\n","    # Diagnal augment\n","    if kwargs['DiagA']:\n","      # add self-loop to edg list -- add 1 connection for each (i,i)\n","      self_loops = np.column_stack((np.arange(n), np.arange(n), np.ones(n)))\n","      # faster than vstack --  adding the second to the bottom\n","      X = np.concatenate((X,self_loops), axis = 0)\n","    \n","    # Laplacian \n","    s = X.shape[0] # get the row number of the edg list\n","    if kwargs[\"Laplacian\"]:\n","      D = np.zeros((n,1))\n","      for row in X:\n","        [v_i, v_j, edg_i_j] = row\n","        v_i = int(v_i)\n","        v_j = int(v_j)\n","        D[v_i] = D[v_i] + edg_i_j\n","        if v_i != v_j:\n","          D[v_j] = D[v_j] + edg_i_j\n","\n","      D = np.power(D, -0.5)\n","      \n","      for i in range(s):\n","        X[i,2] = X[i,2] * D[int(X[i,0])] * D[int(X[i,1])]\n","    \n","    return X\n","\n"," \n","  def adj_to_edg(self,A):\n","    \"\"\"\n","      input is the symmetric adjacency matrix: A\n","      other variables in this function:\n","      s: number of edges\n","      return edg_list -- matrix format with shape(edg_sum,3):\n","      example row in edg_list(matrix): [vertex1, vertex2, connection weight from Adj matrix]\n","    \"\"\"\n","    # check the len of the second dimenson of A\n","    if A.shape[1] <= 3:\n","      edg = A\n","    else:\n","      n = A.shape[0]\n","      # construct the initial edgg_list matrix with the size of (edg_sum, 3)\n","      edg_list = []\n","      for i in range(n):\n","        for j in range(i, n):\n","          if A[i,j] > 0:\n","            row = [i, j, A[i,j]]\n","            edg_list.append(row)\n","      edg = np.array(edg_list)\n","    return edg\n","\n"," \n","  def semi_supervise_preprocess(self):\n","    \"\"\"\n","      get Z, W using multi_graph_encoder_embed()\n","      get training sets and testing sets for Z and Y by using split_data()\n","      \n","    \"\"\"\n","    DataSets =  copy.deepcopy(self)\n","    Y = DataSets.Y\n","    kwargs = DataSets.kwargs\n","    Encoder_kwargs = {k: kwargs[k] for k in ['Correlation']}\n","    # semisupervise do embedding during the learning process\n","    # this timer is only for the first embedding for the normalized input X\n","    embed_time_main_begin = time.time()\n","    if kwargs[\"emb_opt\"] == \"AEE\":\n","      (DataSets.Z, DataSets.W) = multi_graph_encoder_embed(DataSets, Y, **Encoder_kwargs)\n","    if kwargs[\"emb_opt\"] == \"Node2Vec\":\n","      DataSets.Z = node2vec_embed(DataSets.X)  \n","    \n","    embed_time_main_end = time.time()\n","    embed_time_main = embed_time_main_end - embed_time_main_begin\n","    \n","    DataSets.k = DataSets.get_k() \n","    DataSets = DataSets.split_data()\n","    DataSets.embed_time = DataSets.embed_time + embed_time_main\n","  \n","    return DataSets\n","\n","\n","  def get_k(self):\n","    Y = self.Y\n","    n = self.n\n","    # get class number k or the largest cluster size \n","    # max of all flattened element + 1  \n","    if len(Y) == n:\n","      k = np.amax(Y) + 1   \n","    return k\n","\n","\n","  def split_data(self):\n","    split_Sets =  copy.deepcopy(self)\n","   \n","    Y = split_Sets.Y\n","    Z = split_Sets.Z\n","    \n","    ind_train = np.argwhere (Y >= 0)[:,0]\n","    ind_unlabel = np.argwhere (Y < 0)[:,0]   \n","\n","    Y_train = Y[ind_train, 0]\n","    z_train = Z[ind_train]\n","\n","    Y_unlabel = None\n","    z_unlabel = None\n","\n","    if len(ind_unlabel) > 0:\n","      Y_unlabel = Y[ind_unlabel, 0]\n","      z_unlabel = Z[ind_unlabel] \n","\n","    # Convert targets into one-hot encoded format      \n","    Y_train_one_hot = to_categorical(Y_train) \n","\n","    split_Sets.ind_unlabel = ind_unlabel\n","    split_Sets.ind_train = ind_train    \n","    split_Sets.Y_train = Y_train\n","    split_Sets.Y_unlabel = Y_unlabel\n","    split_Sets.z_train = z_train\n","    split_Sets.z_unlabel = z_unlabel\n","    split_Sets.Y_train_one_hot = Y_train_one_hot\n","\n","    return split_Sets\n","\n","\n","  def DataSets_reset(self, option):\n","    \"\"\"\n","      based on the information of the given new Y:\n","      1. reassign Z and W to the given DataSets, \n","      2. update z_train, z_unlabel \n","      Input Option:\n","      1. if the option is \"y_temp\", do graph encoder using y_temp\n","    \"\"\"\n","    NewSets =  copy.deepcopy(self)\n","    kwargs = NewSets.kwargs\n","    ind_unlabel = NewSets.ind_unlabel\n","    ind_train = NewSets.ind_train   \n","    y_temp =  NewSets.y_temp\n","    Encoder_kwargs = {k: kwargs[k] for k in ['Correlation']}\n","\n","    # different versions\n","    if option == \"y_temp\":\n","      [Z,W] = multi_graph_encoder_embed(NewSets, y_temp, **Encoder_kwargs)\n","    if option == \"y_temp_one_hot\":\n","      y_temp_one_hot = NewSets.y_temp_one_hot\n","      [Z,W] = multi_graph_encoder_embed(NewSets, y_temp_one_hot, **Encoder_kwargs)  \n","    if NewSets.attributes:\n","      # add U to Z side by side\n","      Z = np.concatenate((Z, NewSets.U), axis=1)  \n","    \n","    NewSets.Z = Z\n","    NewSets.W = W\n","    NewSets.z_train = Z[ind_train]\n","    NewSets.z_unlabel = Z[ind_unlabel]\n","    \n","    return NewSets\n","\n","  \n","  def supervise_preprocess(self):\n","    \"\"\"\n","      adding test sets for supervised learning    \n","      this function assumes only one test set\n","      if there is a list of test set, needs to modify this function\n","    \"\"\"\n","\n","    DataSets = self.semi_supervise_preprocess()\n","    Dataset_input = DataSets.Dataset_input\n","\n","    DataSets.z_test = DataSets.Z[Dataset_input.test_idx]\n","    DataSets.Y_test = Dataset_input.Y_test.ravel()\n","    DataSets.z_unlabel = None\n","    DataSets.Y_unlabel = None\n","\n","    return DataSets\n","############------------DataPreprocess_end-----------------------###############\n","\n","############-----------------GNN_start---------------------------###############\n","def batch_generator(X, y, k, batch_size, shuffle):\n","    number_of_batches = int(X.shape[0]/batch_size)\n","    counter = 0\n","    sample_index = np.arange(X.shape[0])\n","    if shuffle:\n","        np.random.shuffle(sample_index)\n","    while True:\n","        batch_index = sample_index[batch_size*counter:batch_size*(counter+1)]\n","        X_batch = X[batch_index,:]\n","        y_batch = y[batch_index,:]\n","        counter += 1\n","        yield X_batch, y_batch\n","        if (counter == number_of_batches):\n","            if shuffle:\n","                np.random.shuffle(sample_index)\n","            counter = 0\n","\n","class Hyperperameters:\n","  \"\"\"\n","    define perameters for GNN.\n","    default values are for GNN learning -- \"Leaner\" ==2:\n","      embedding via partial label, then learn unknown label via two-layer NN\n","\n","  \"\"\"\n","  def __init__(self):\n","    # there is no scaled conjugate gradiant in keras optimiser, use defualt instead\n","    # use whatever default\n","    self.learning_rate = 0.01  # Initial learning rate.\n","    self.epochs = 100 #Number of epochs to train.\n","    self.hidden = 20 #Number of units in hidden layer \n","    self.val_split = 0.1 #Split 10% of training data for validation\n","    self.loss = 'categorical_crossentropy' # loss function\n","\n","\n","class GNN:\n","  def __init__(self, DataSets, **kwargs):\n","    GNN.kwargs = self.kwargs_construct(**kwargs)\n","    GNN.DataSets = DataSets\n","    GNN.hyperM = Hyperperameters()\n","    GNN.model = self.GNN_model()  #model summary: GNN.model.summary()\n","    GNN.meanSS = 0  # initialize the self-defined critirion meanSS\n","    \n","  def kwargs_construct(self, **kwargs):\n","    defaultKwargs = {'Learner': 1,                    # GNN_Leaner\n","                     'LearnerIter': 0,                # GNN_complete, GNN_Iter\n","                     \"Replicates\": 3,                 # GNN_Iter  \n","                     \"Batch_input\": False              # if run in batches       \n","                     }\n","    kwargs = { **defaultKwargs, **kwargs}  # update the args using input_args\n","    return kwargs      \n"," \n","  \n","  def GNN_model(self):\n","    \"\"\"\n","      build GNN model\n","    \"\"\"\n","    hyperM = self.hyperM\n","    DataSets = self.DataSets\n","\n","    z_train = DataSets.z_train\n","    k = DataSets.k\n","\n","    feature_num = z_train.shape[1]\n","    \n","    model = keras.Sequential([\n","    keras.layers.Flatten(input_shape = (feature_num,)),  # input layer \n","    keras.layers.Dense(hyperM.hidden, activation='relu'),  # hidden layer -- no tansig activation function in Keras, use relu instead\n","    keras.layers.Dense(k, activation='softmax') # output layer, matlab used softmax for patternnet default ??? max(opts.neuron,K)? opts \n","    ])\n","\n","    optimizer = keras.optimizers.Adam(learning_rate = hyperM.learning_rate)\n","\n","    model.compile(optimizer='adam',\n","                  loss=hyperM.loss,\n","                  metrics=['accuracy'])\n","\n","    return model\n","  \n","\n","  def GNN_run(self, k, z_train, y_train_one_hot, z_unlabel):\n","    \"\"\"\n","      Train and test directly.\n","      Do not learn from the unknown labels.\n","    \"\"\"\n","    gnn = copy.deepcopy(self)\n","    hyperM = gnn.hyperM\n","    model = gnn.model    \n","    batch_flag = self.kwargs[\"Batch_input\"]\n","    \n","    if batch_flag:\n","      early_stopping_callback = EarlyStopping(monitor='loss', patience=5, verbose=0)\n","      checkpoint_callback = ModelCheckpoint('GNN.h5', monitor='loss', save_best_only=True, mode='min', verbose=0)\n","      \n","      history = model.fit(batch_generator(z_train, y_train_one_hot, k, 32, True),\n","                      epochs=hyperM.epochs,\n","                      steps_per_epoch=z_train.shape[0],\n","                      callbacks=[early_stopping_callback, checkpoint_callback],\n","                      verbose=0)\n","    else:\n","      # validation_split=hyperM.val_split\n","      history = model.fit(z_train, y_train_one_hot, \n","            validation_split=hyperM.val_split,\n","            epochs=hyperM.epochs, \n","            shuffle=True,\n","            verbose=0)\n","    \n","    train_acc = history.history['accuracy'][-1]\n","    \n","    predict_probs = None\n","    pred_class = None\n","    pred_class_prob = None\n","    if type(z_unlabel) == np.ndarray:\n","      # predict_probas include probabilities for all classes for each node\n","      predict_probs = model.predict(z_unlabel)\n","      # assign the classes with the highest probability\n","      pred_class = np.argmax(predict_probs, axis=1)\n","      # the corresponding probabilities of the predicted classes\n","      pred_class_prob = predict_probs[range(len(predict_probs)),pred_class]\n","\n","    gnn.model = model\n","    gnn.train_acc = train_acc\n","    gnn.pred_probs = predict_probs  \n","    gnn.pred_class = pred_class\n","    gnn.pred_class_prob = pred_class_prob\n","\n","\n","    return gnn\n","\n","  def GNN_Direct(self, DataSets, y_train_one_hot):\n","    \"\"\"\n","      This function can run:\n","      1. by itself, when interation is set to False (<1)\n","      2. inside GNN_Iter, when interation is set to True (>=1)\n","\n","      Learner == 0: GNN, but not learn from the known label\n","      Learner == 2: GNN, and learn unknown labels \n","    \"\"\"\n","    Learner = self.kwargs[\"Learner\"]  \n","\n","    k = DataSets.k \n","    z_train = DataSets.z_train \n","    Y = DataSets.Y\n","    z_unlabel = DataSets.z_unlabel\n","    ind_unlabel = DataSets.ind_unlabel\n","\n","    gnn = self.GNN_run(k, z_train, y_train_one_hot, z_unlabel)\n","\n","    if Learner == 0:\n","      # do not learn unknown label.\n","      pass\n","    \n","    if Learner == 2:\n","      # learn unknown label based on the known label\n","      # replace the unknown label in Y with predicted labels\n","      pred_class = gnn.pred_class\n","      Y[ind_unlabel, 0] = pred_class\n","\n","    gnn.Y = Y\n","\n","    return gnn\n","\n","\n","  def GNN_Iter(self, DataSets):\n","    \"\"\"\n","      Run this function when interation is set, which is >=1.\n","      \n","      1. randomly assign labels to the unknown labels, get Y_temp\n","      2. get Y_one_hot for the Y_temp \n","      3. get Z from graph_encod function with X and Y_temp \n","      within each loop: \n","        use meanSS as its criterion to decide if the update is needed        \n","\t      update Y_one_hot for the unknown labels with predict probabilities of each classes\n","\t      update Y with the highest possible predicted labels\n","\t      update z_train and z_unlabel from graph encoder embedding using the updated Y\n","\t      train the model with the updated z_train and Y_one_hot     \n","    \"\"\"\n","\n","    kwargs = self.kwargs\n","    meanSS = self.meanSS\n","\n","    k = DataSets.k \n","    Y = DataSets.Y\n","    ind_unlabel = DataSets.ind_unlabel\n","\n","\n","    y_temp = np.copy(Y)\n","    DataSets.y_temp = y_temp\n","\n","\n","    for i in range(kwargs[\"Replicates\"]):\n","      # assign random integers in [1,K] to unassigned labels\n","      r = [i for i in range(k)]\n","      \n","      ran_int = np.random.choice(r, size=(len(ind_unlabel),1))\n","\n","      y_temp[ind_unlabel] = ran_int\n","\n","      for j in range(kwargs[\"LearnerIter\"]):\n","        if j ==0:\n","          # first iteration need to split the y_temp for training etc.\n","          # use reset to add z_train, z_unlabel, y_temp_one_hot, to the dataset\n","          DataSets = DataSets.DataSets_reset(\"y_temp\")  \n","          # Convert targets into one-hot encoded format      \n","          y_temp_one_hot = to_categorical(y_temp) \n","          # initialize y_temp_one_hot in the first loop\n","          DataSets.y_temp_one_hot = y_temp_one_hot     \n","        if j > 0:\n","          # update z_train, z_unlabel, and y_temp_train_one_hot to the dataset\n","          DataSets = DataSets.DataSets_reset(\"y_temp\")\n","        # all the gnn train on y_train_one_hot\n","        gnn = self.GNN_Direct(DataSets, DataSets.Y_train_one_hot)\n","        predict_probs = gnn.pred_probs\n","        pred_class = gnn.pred_class\n","        pred_class_prob = gnn.pred_class_prob\n","\n","        # z_unknown is initialized with none, so the pred_class may be none\n","        # This will not happen for the semi version,\n","        # since the unknown size should not be none for the semi version\n","        if type(pred_class) == np.ndarray:\n","          # if there are unkown labels and predicted labels are available\n","          # check if predicted_class are the same as the random integers\n","          # if so, stop the iteration in \"LearnerIter\" loop\n","          # shape (n,) is required for adjusted_rand_score()\n","          if adjusted_rand_score(ran_int.reshape((-1,)), pred_class) == 1:\n","            break\n","          # assign the probabilites for each class to the temp y_one_hot\n","          DataSets.y_temp_one_hot[ind_unlabel] = predict_probs\n","          # assgin the predicted classes to the temp Y unknown labels \n","          DataSets.y_temp[ind_unlabel, 0] = pred_class \n","          # # assign the highest possibility of the class to Y_temp\n","          # Y_temp[ind_unlabel, 0] = pred_class_prob\n","      minP = np.mean(pred_class_prob) - 3*np.std(pred_class_prob)\n","      if minP > meanSS:\n","        meanSS = minP\n","        Y = DataSets.y_temp   \n","\n","      gnn.Y = Y\n","      gnn.meanSS = meanSS\n","      return gnn  \n","  \n","        \n","  def GNN_complete(self):\n","    \"\"\"\n","      if LearnerIter set to False(<1):\n","        run GNN_Direct() with no iteration\n","      if LearnerIter set to True(>=1):\n","        run GNN_Iter(), which starts with radomly assigned k to unknown labels\n","      \n","    \"\"\"\n","    kwargs = self.kwargs\n","\n","    DataSets = self.DataSets\n","    y_train = DataSets.Y_train\n","\n","\n","    if kwargs[\"LearnerIter\"] < 1:\n","      # Convert targets into one-hot encoded format\n","      y_train_one_hot = to_categorical(y_train)\n","      gnn = self.GNN_Direct(DataSets, y_train_one_hot)\n","    else:\n","      gnn = self.GNN_Iter(DataSets)\n","    \n","    return gnn\n","############-----------------GNN_end-----------------------------###############\n","\n","############-----------------LDA_start---------------------------###############\n","class LDA:\n","  def __init__(self, DataSets, **kwargs):\n","    LDA.kwargs = self.kwargs_construct(**kwargs)\n","    LDA.DataSets = DataSets\n","    LDA.model = LinearDiscriminantAnalysis()  # asssume spseudolinear is its default setting\n","    LDA.meanSS = 0  # initialize the self-defined critirion meanSS\n","    \n","  def kwargs_construct(self, **kwargs):\n","    defaultKwargs = {'Learner': 1,                         # LDA_Leaner\n","                     'LearnerIter': 0, \"Replicates\": 3     # LDA_Iter                           \n","                     }\n","    kwargs = { **defaultKwargs, **kwargs}  # update the args using input_args\n","    return kwargs   \n","\n","  def LDA_Learner(self, DataSets):\n","    \"\"\"\n","      run this function when Learner set to 1.\n","      embedding via partial label, then learn unknown label via LDA.\n","    \"\"\"  \n","    lda = copy.deepcopy(self)\n","    z_train = DataSets.z_train\n","    y_train = DataSets.Y_train\n","    ind_unlabel = DataSets.ind_unlabel\n","    z_unlabel = DataSets.z_unlabel\n","    Y = DataSets.Y\n","\n","    model = self.model\n","    model.fit(z_train,y_train)\n","    # train_acc = model.score(z_train,y_train)\n","\n","    # for semi-supervised learning \n","    if type(z_unlabel) == np.ndarray:\n","      # predict_probas include probabilities for all classes for each node\n","      pred_probs = model.predict_proba(z_unlabel)\n","      # assign the classes with the highest probability\n","      pred_class = model.predict(z_unlabel)\n","      # the corresponding probabilities of the predicted classes\n","      pred_class_prob = pred_probs[range(len(pred_probs)),pred_class] \n","      # assign the predicted class to Y\n","      Y[ind_unlabel, 0] = pred_class\n","      lda.Y = Y\n","      lda.pred_class = pred_class\n","      lda.pred_class_prob = pred_class_prob\n","\n","    lda.model = model\n","    return lda\n","    \n","  def LDA_Iter(self):\n","    \"\"\"\n","      run this function when Learner set to 1, and LeanerIter is True(>=1)\n","      ramdonly assign labels to the unknownlabel.\n","      embedding via partial label, then learn unknown label via LDA. \n","    \"\"\"\n","\n","    kwargs = self.kwargs\n","    meanSS = self.meanSS\n","    DataSets = self.DataSets\n","\n","    k = DataSets.k \n","    Y = DataSets.Y\n","    ind_unlabel = DataSets.ind_unlabel\n","\n","    y_temp = np.copy(Y)\n","\n","    for i in range(kwargs[\"Replicates\"]):\n","      # assign random integers in [1,K] to unassigned labels\n","      r = [i for i in range(k)]\n","      \n","      ran_int = np.random.choice(r, size=(len(ind_unlabel),1))\n","\n","      y_temp[ind_unlabel] = ran_int\n","      \n","      DataSets.y_temp = y_temp\n"," \n","      for j in range(kwargs[\"LearnerIter\"]):\n","        # use reset to add z_train, z_unlabel, to the dataset\n","        DataSets = DataSets.DataSets_reset(\"y_temp\")         \n","        # all train on y_train\n","        lda = self.LDA_Learner(DataSets)\n","        pred_class = lda.pred_class\n","        pred_class_prob = lda.pred_class_prob\n","\n","        # z_unknown is initialized with none, so the pred_class may be none\n","        # This will not happen for the semi version,\n","        # since the unknown size should not be none for the semi version\n","        if type(pred_class) == np.ndarray:\n","          # if there are unkown labels and predicted labels are available\n","          # check if predicted_class are the same as the random integers\n","          # if so, stop the iteration in \"LearnerIter\" loop\n","          # shape (n,) is required for adjusted_rand_score()\n","          if adjusted_rand_score(ran_int.reshape((-1,)), pred_class) == 1:\n","            break\n","          # assgin the predicted classes to the temp Y unknown labels \n","          DataSets.y_temp[ind_unlabel, 0] = pred_class \n","          # # assign the highest possibility of the class to Y_temp\n","          # Y_temp[ind_unlabel, 0] = pred_class_prob\n","      minP = np.mean(pred_class_prob) - 3*np.std(pred_class_prob)\n","      if minP > meanSS:\n","        meanSS = minP\n","        Y = DataSets.y_temp   \n","\n","      lda.Y = Y\n","      lda.meanSS = meanSS\n","      return lda  \n","  \n","\n","############-----------------LDA_end-----------------------------###############\n","\n","############------------Clustering_start-------------------------###############\n","class Clustering:\n","  \"\"\"\n","    The input DataSets.X is the s*3 edg list\n","    The innput DataSets.Y can be:\n","    1. A given cluster size, e.g. [3], meaning in total 3 clusters\n","    2. A range of cluster sizes. e.g. [3-5], meaning there are possibly 3 to 5 clusters \n","\n","  \"\"\"\n","  def __init__(self, DataSets, **kwargs):\n","    self.kwargs = self.kwargs_construct(**kwargs)\n","    self.DataSets = DataSets\n","    self.cluster_size_range = self.cluster_size_check() \n","    self.K = DataSets.Y[0]\n","  \n","\n","  def kwargs_construct(self, **kwargs):\n","    defaultKwargs = {'Correlation': True,'MaxIter': 50, 'MaxIterK': 5,'Replicates': 3}\n","    kwargs = { **defaultKwargs, **kwargs}\n","    return kwargs\n","\n","  def cluster_size_check(self):\n","    DataSets = self.DataSets\n","    Y = DataSets.Y\n","\n","    cluster_size_range = None # in case that Y is an empty array.\n","\n","    if len(Y) == 1:\n","      cluster_size_range = False  # meaning the cluster size is known. e.g. [3]\n","    if len(Y) > 1:\n","      cluster_size_range = True   # meaning only know the range of cluster size. e.g. [2, 3, 4, 5] \n","    \n","    return cluster_size_range\n","    \n","  def graph_encoder_cluster(self, K):\n","    \"\"\"\n","      clustering function\n","    \"\"\"\n","    DataSets = self.DataSets\n","    X = DataSets.X\n","    n = DataSets.n\n","    kwargs = self.kwargs\n","    Encoder_kwargs = {k: kwargs[k] for k in ['Correlation']}\n","\n","\n","    minSS=-1\n","    Z = None\n","    W = None\n","\n","    for i in range(kwargs['Replicates']):\n","      Y_temp = np.random.randint(K,size=(n,1))\n","      for r in range(kwargs['MaxIter']):\n","        [Zt,Wt] = multi_graph_encoder_embed(DataSets, Y_temp, **Encoder_kwargs)  \n","        \n","        if DataSets.attributes:\n","          # add U to Z side by side\n","          Zt = np.concatenate((Zt, DataSets.U), axis=1)\n","        kmeans = KMeans(n_clusters=K, max_iter = kwargs['MaxIter']).fit(Zt)\n","        labels = kmeans.labels_ # shape(n,)\n","        # sum_in_cluster = kmeans.inertia_ # sum of distance within cluster (k,1)\n","        dis_to_centors = kmeans.transform(Zt)\n","        # adjusted_rand_score() needs the shape (n,)\n","        if adjusted_rand_score(Y_temp.reshape(-1,), labels) == 1:\n","          break\n","        else:\n","          # we need labels to be the same shape as for Y(n,1) when assign\n","          Y_temp = labels.reshape(-1,1) \n","      \n","      # calculate score and compare with meanSS\n","      tmp = self.temp_score(dis_to_centors, K, labels, n)\n","      if (minSS == -1) or tmp < minSS:\n","        Z = Zt\n","        W = Wt\n","        minSS = tmp\n","        Y = labels\n","    return  Z, Y, W, minSS\n","\n","\n","  def temp_score(self, dis_to_centors, K, labels, n):\n","    \"\"\"\n","      calculate:\n","      1. sum_in_cluster(1*k): the sum of the distance from the nodes to the centroid \n","      of its belonged cluster\n","      2. sum_in_cluster_norm(1*k): normalize the sum_in_cluster by the \n","      corresponding label count (how many nodes in each cluster)\n","      3. sum_not_in_cluster(1*k): the sum of the distance of the cluster \n","      centroid to the nodes that do not belong to the cluster\n","      4. sum_not_in_cluster_norm(1*k): normalize the sum_other_centroids by the \n","      counts of the nodes that do not belong to the cluster.\n","      5. temp score(1*k): \n","      (normalized sum in cluster / normalized sum not in cluster ) *\n","      (label count in cluster / total node number)\n","      6. get mean + 2 standard deviation of temp score, then return \n","    \"\"\"\n","    label_count = np.bincount(labels)\n","    sum_in_cluster_squre = np.zeros((K,))\n","\n","    dis_to_centors_squre = dis_to_centors**2\n","\n","    for i in range(n):\n","      label = labels[i]\n","      sum_in_cluster_squre[label] += dis_to_centors_squre[i][label]\n","    \n","    # how to find out if the distance is squared, the current method doesn't do square root.\n","    sum_not_in_cluster = (np.sum(dis_to_centors_squre, axis=0) - sum_in_cluster_squre)**0.5\n","\n","    sum_not_in_cluster_norm = sum_not_in_cluster/(n - label_count)\n","    sum_in_cluster_norm = sum_in_cluster_squre**0.5/label_count\n","\n","    tmp = sum_in_cluster_norm / sum_not_in_cluster_norm * label_count / n\n","    tmp = np.mean(tmp) + 2*np.std(tmp)\n","\n","    return tmp\n","\n","\n","  def cluster_main(self):\n","    K = self.K\n","    DataSets = self.DataSets    \n","    X = DataSets.X\n","    n = DataSets.n\n","\n","    kmax = np.amax(K)\n","    if n/kmax < 30:\n","      print('Too many clusters at maximum. Result may bias towards large K. Please make sure n/Kmax >30.')\n","    # when the cluster size is specified\n","    if not self.cluster_size_range:\n","      [Z,Y,W,meanSS]= self.graph_encoder_cluster(K[0])\n","    # when the range of cluster size is provided \n","    # columns are less than n/2 and kmax is less than max(n/2, 10)\n","    if self.cluster_size_range:\n","      k_range = len(K)\n","      if k_range < n/2 and kmax < max(n/2, 10):\n","          minSS = -1\n","          Z = 0\n","          W = 0\n","          meanSS = np.zeros((k_range,1))\n","          for i in range(k_range):\n","            [Zt,Yt,Wt,tmp]= self.graph_encoder_cluster(K[i])\n","            meanSS[i,0] = i\n","            if (minSS == -1) or tmp < minSS:\n","              minSS = tmp\n","              Y = Yt\n","              Z = Zt\n","              W = Wt\n","    return Z, Y, W, meanSS\n","\n","############------------Clustering_end---------------------------###############\n","############------------Evaluation_start---------------------------#############\n","class Evaluation:\n","  def GNN_supervise_test(self, gnn, z_test, y_test):\n","    \"\"\"\n","      test the accuracy for GNN_direct\n","    \"\"\"\n","    y_test_one_hot = to_categorical(y_test) \n","    # set verbose to 0 to silent the output\n","    test_loss, test_acc = gnn.model.evaluate(z_test,  y_test_one_hot, verbose=0) \n","\n","    return test_acc\n","\n","  def LDA_supervise_test(self, lda, z_test, y_test):\n","    \"\"\"\n","      test the accuracy for LDA_learner\n","    \"\"\"\n","    test_acc = lda.model.score(z_test, y_test)\n","\n","    return test_acc\n","\n","  def GNN_semi_supervised_learn_test(self,Y_result, Y_original):\n","    \"\"\"\n","      test accuracy for semi-supervised learning\n","    \"\"\"\n","    test_acc = metrics.accuracy_score(Y_result, Y_original)   \n","\n","    return test_acc\n","\n","  def GNN_semi_supervised_not_learn_test(self, gnn, Dataset, case):\n","    \"\"\"\n","      test accuracy for semi-supervised learning\n","    \"\"\"\n","\n","    ind_unlabel = Dataset.ind_unlabel\n","    z_unlabel =  Dataset.z_unlabel \n","    y_unlabel_ori = case.Y_ori[ind_unlabel, 0]\n","    y_unlabel_ori_one_hot = to_categorical(y_unlabel_ori) \n","    test_loss, test_acc = gnn.model.evaluate(z_unlabel, y_unlabel_ori_one_hot, verbose=0)\n","\n","    return test_acc\n","\n","\n","  def clustering_test(self, Y_result, Y_original):\n","    \"\"\"\n","      test accuracy for semi-supervised learning\n","    \"\"\"\n","    ari = adjusted_rand_score(Y_result, Y_original.reshape(-1,))\n","\n","    return ari\n","\n","############-----------------Matrix_conversion-------------------###############\n","def To_multi_sparse_matrix(M_list,option):\n","  M_list_new = []\n","  for M in M_list:\n","    M_new = To_single_sparse_matrix(M,option)\n","    M_list_new.append(M_new)\n","\n","  return M_list_new\n","\n","def To_single_sparse_matrix(M,option):\n","  \"\"\"\n","    coo_matrix is efficient and fast to construct, \n","      However, arithmetic operations are not efficient on this matrix. \n","      One can easily convert coo_matrix to csc_matrix/csr_matrix \n","    csc_matrix/csr_matrix are efficient in column_slicing/row_slicing,\n","      One can have efficient multiplication or inversion.\n","  \"\"\"\n","  if option == \"coo\":\n","    M = sparse.coo_matrix(M)\n","  if option == \"csr\":\n","    M = sparse.csr_matrix(M)\n","  if option == \"csc\":\n","    M = sparse.csc_matrix(M)\n","  \n","  return M"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["import math\n","import copy\n","import networkx as nx\n","\n","class RealDataSet:\n","  def __init__(self, edg_file, node_file):\n","    self.X = None  # edg_list\n","    self.n = None\n","    self.Y = None\n","    self.edg_file = edg_file\n","    self.node_file = node_file\n","  \n","  def get_initial_values(self):\n","    realSet = copy.deepcopy(self)\n","\n","    label_dict, map_new_old_keys = self.read_node_file(self.node_file)\n","    n = self.get_n(label_dict)\n","    \n","    if map_new_old_keys:\n","      X = self.read_edge_file_with_remap(self.edg_file, n, map_new_old_keys)\n","    else:\n","      X = self.read_edge_file(self.edg_file, n)\n","    \n","    realSet.X = X\n","    label_dict = self.check_class_values(label_dict)\n","    Y = self.get_labels(label_dict, n)\n","    realSet.Y = Y\n","    realSet.n = n\n","    realSet.k = self.get_k(label_dict)\n","\n","    return realSet\n","  \n","  def check_class_values(self, label_dict):\n","    \"\"\"\n","      check if class values start with 0, if not, correct it\n","    \"\"\"\n","    \n","    if min(set(label_dict.values())) != 0:\n","      for k, v in label_dict.items():\n","         label_dict[k] = str(int(v) - 1)\n","    \n","    return label_dict\n","\n","  def find_split_point(self, firstline):\n","    # find split point\n","    split_point_pos = [\",\", \"\\t\", \" \"]\n","    split_point = \"\"\n","    for sp in split_point_pos:\n","      if sp in firstline:\n","        split_point = sp\n","        break\n","    return split_point\n","\n","  def read_node_file(self, filename):\n","    \"\"\"\n","      the node in the node file start with node 1 not node 0\n","    \"\"\"\n","    re_map_node = False\n","    label_dict = {}\n","    labels = open(filename, \"r\") \n","    line_count = 0\n","    map_new_old_keys = {}\n","    \n","    for l in labels:\n","      line_count += 1\n","      if (line_count) == 1:\n","        split_point = self.find_split_point(l)\n","      (node_i, label_i) = l.strip().split(split_point)\n","      if (line_count) == 1 and (int(node_i) != 1):\n","        re_map_node = True \n","      label_dict[int(node_i)-1] = label_i \n","    # if there is an id for the node, for example PMID for pubmed data\n","    # need to map the pubmed id back to a serie of node IDs starting from 0 \n","    if re_map_node:\n","      keys = sorted(list(label_dict.keys()))\n","      new_node_idx = [i for i in range(len(keys))]\n","      new_label_dict = {}\n","      for i in range(len(keys)):\n","        map_new_old_keys[keys[i]] = new_node_idx[i]\n","        new_label_dict[new_node_idx[i]] = label_dict[keys[i]]\n","      label_dict = new_label_dict\n","        \n","    return label_dict, map_new_old_keys\n","  \n","  def get_n(self, label_dict):\n","    \"\"\"\n","      get the number of nodes: n\n","      the keys start with 0, so n is max + 1.\n","    \"\"\"\n","    n = max(sorted(list(label_dict.keys())))+1\n","    return n\n","  \n","  def get_k(self, label_dict):\n","    \"\"\"\n","      get the number of classes: k\n","    \"\"\"\n","    k = len(set(label_dict.values()))\n","    return k\n","\n","  def read_edge_file(self, filename, n):\n","    \"\"\"\n","      NOTE: the node in the node file start with node 1 not node 0\n","    \"\"\"\n","    edg_list = []\n","    edges = open(filename, \"r\") \n","\n","    line_count = 0\n","    for l in edges:\n","      \n","      line_count += 1\n","      if (line_count) == 1:\n","        split_point = self.find_split_point(l)\n","\n","      elements = l.strip().split(split_point)\n","      if len(elements) > 2:\n","        (node_i, node_j, w) = elements\n","        edg_list.append([int(node_i)-1, int(node_j)-1, float(w)])\n","      else: \n","        (node_i, node_j) = elements\n","        edg_list.append([int(node_i)-1, int(node_j)-1, 1]) \n","    edg = np.array(edg_list)\n","    return edg  \n","\n","  def read_edge_file_with_remap(self, filename, n, map_new_old_keys):\n","    \"\"\"\n","      for the ids that are remaped from the node file, \n","      need to remap id for edge list as well\n","    \"\"\"\n","    edg_list = []\n","    edges = open(filename, \"r\") \n","\n","    line_count = 0\n","    for l in edges:\n","      line_count += 1\n","      if (line_count) == 1:\n","        split_point = self.find_split_point(l)\n","      elements = l.strip().split(split_point)\n","      if len(elements) > 2:\n","        (node_i, node_j, w) = elements\n","        new_idx_i = map_new_old_keys[int(node_i)-1]\n","        new_idx_j = map_new_old_keys[int(node_j)-1]\n","        edg_list.append([new_idx_i, new_idx_j, float(w)])\n","      else: \n","        (node_i, node_j) = elements\n","        new_idx_i = map_new_old_keys[int(node_i)-1]\n","        new_idx_j = map_new_old_keys[int(node_j)-1]        \n","        edg_list.append([new_idx_i, new_idx_j, 1]) \n","    edg = np.array(edg_list)\n","    return edg  \n","\n","  def check_label(self, label_dict, n):\n","    \"\"\"\n","      the input label_dict start with key 0\n","    \"\"\"\n","    check = True\n","    keys = sorted(list(label_dict.keys()))\n","    unlabeld_node_idx = []\n","    for node_idx in range(n):\n","      if node_idx not in keys:\n","        unlabeld_node_idx.append(node_idx)\n","    if len(unlabeld_node_idx) > 0:\n","      print(\"There are node(s) not labeled\")\n","      check = False\n","    return check, unlabeld_node_idx\n","\n","  def get_labels(self, label_dict, n):\n","    check, unlabeld_node_idx = self.check_label(label_dict, n)\n","    keys = sorted(list(label_dict.keys()))\n","    Y = np.zeros((n,1), dtype=int)\n","    for node_idx in keys:    \n","      Y[node_idx][0] = int(label_dict[node_idx])\n","    if not check:\n","      for idx in unlabeld_node_idx:\n","        Y[idx][0] = -1\n","\n","    return Y\n","\n","  def split_sets(self, test_ratio):\n","\n","    DataSet = copy.deepcopy(self)\n","    Y_ori = DataSet.Y\n","    Y = np.copy(Y_ori)\n","\n","    t = test_ratio\n","    Y_1st_dim = Y.shape[0]\n","\n","    np.random.seed(0)\n","    indices = np.random.permutation(Y_1st_dim)  #randomly permute the 1st indices\n","\n","    # Generate indices for splits\n","    test_ind_split_point = math.floor(Y_1st_dim*t)\n","    test_idx, train_idx = indices[:test_ind_split_point], indices[test_ind_split_point:]\n","\n","    \n","    # get the Y_test label\n","    Y_test = Y[test_idx]\n","    Y_train = Y[train_idx]\n","    # mark the test position as unknown: -1\n","    Y[test_idx, 0] = -1    \n","\n","\n","    DataSet.Y_ori = Y_ori\n","    DataSet.Y = Y\n","    DataSet.Y_train = Y_train.ravel()\n","    DataSet.Y_test = Y_test.ravel() \n","    DataSet.test_idx = test_idx\n","    DataSet.train_idx = train_idx    \n","    return DataSet \n","\n","def edge_list_to_adjacency_matrix(edg_list, n):\n","  A = np.zeros((n,n))\n","  for [i, j, w] in edg_list:\n","    i = int(i)\n","    j = int(j)\n","    if A[i,j] != w:\n","      A[i,j] = w\n","    if A[j,i] != w:\n","      A[j,i] = w\n","  return A"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# invalide devide resutls will be nan\n","np.seterr(divide='ignore', invalid='ignore')\n","\n","############------------graph_encoder_embed_start----------------###############\n","class GraphEncoderEmbed:\n","  def run(self, X, Y, n, **kwargs):\n","    defaultKwargs = {'EdgeList': False, 'DiagA': True, 'Laplacian': False, 'Correlation': True}\n","    kwargs = { **defaultKwargs, **kwargs}\n","\n","    if kwargs['EdgeList']:\n","      size_flag = self.edge_list_size\n","      X = self.Edge_to_Sparse(X, n, size_flag)\n","    \n","    if kwargs['DiagA']:\n","      X = self.Diagonal(X, n)\n","\n","    if kwargs['Laplacian']:\n","      X = self.Laplacian(X, n)\n","    \n","    Z, W = self.Basic(X, Y, n)\n","\n","    if kwargs['Correlation']:\n","      Z = self.Correlation(Z)\n","    \n","    return Z, W\n","\n","  def Basic(self, X, Y, n):\n","    \"\"\"\n","      graph embedding basic function\n","      input X is sparse csr matrix of adjacency matrix\n","      -- if there is a connection between node i and node j:\n","      ---- X(i,j) = 1, no edge weight\n","      ---- X(i,j) = edge weight.\n","      -- if there is no connection between node i and node j:\n","      ---- X(i,j) = 0, \n","      ---- note there is no storage for this in sparse matrix. \n","      ---- No storage means 0 in sparse matrix.\n","      input Y is numpy array with size (n,1):\n","      -- value -1 indicate no lable\n","      -- value >=0 indicate real label\n","      input train_idx: a list of indices of input X for training set \n","    \"\"\"\n","    # assign k to the max along the first column\n","    # Note for python, label Y starts from 0. Python index starts from 0. thus size k should be max + 1\n","    k = Y[:,0].max() + 1\n","    \n","    #nk: 1*n array, contains the number of observations in each class\n","    nk = np.zeros((1,k))\n","    for i in range(k):\n","      nk[0,i] = np.count_nonzero(Y[:,0]==i)\n","    \n","    #W: sparse matrix for encoder matrix. W[i,k] = {1/nk if Yi==k, otherwise 0}\n","    W = sparse.dok_matrix((n, k), dtype=np.float32)\n","\n","    for i in range(n):\n","      k_i = Y[i,0]\n","      if k_i >=0:\n","        W[i,k_i] = 1/nk[0,k_i]\n","    \n","    W = sparse.csr_matrix(W)\n","    Z = X.dot(W)\n","    \n","    return Z, W\n","\n","  def Diagonal(self, X, n):\n","    \"\"\"\n","      input X is sparse csr matrix of adjacency matrix\n","      return a sparse csr matrix of X matrix with 1s on the diagonal\n","    \"\"\"\n","    I = sparse.identity(n)\n","    X = X + I\n","    return X\n","\n","\n","  def Laplacian(self, X, n):\n","    \"\"\"\n","      input X is sparse csr matrix of adjacency matrix\n","      return a sparse csr matrix of Laplacian normalization of X matrix\n","    \"\"\"\n","    X_sparse = sparse.csr_matrix(X)\n","    # get an array of degrees\n","    dig = X_sparse.sum(axis=0).A1\n","    # diagonal sparse matrix of D\n","    D = sparse.diags(dig,0)\n","    _D = D.power(-0.5)\n","    # D^-0.5 x A x D^-0.5\n","    L = _D.dot(X_sparse.dot(_D)) \n","\n","    # _L = _D.dot(X_sparse.dot(_D))    \n","    # # L = I - D^-0.5 x A x D^-0.5\n","    # I = sparse.identity(n)\n","    # L = I - _L   \n","\n","    return L\n","  \n","  def Correlation(self, Z):\n","    \"\"\"\n","      input Z is sparse csr matrix of embedding matrix from the basic function\n","      return normalized Z sparse matrix\n","      Calculation:\n","      Calculate each row's 2-norm (Euclidean distance). \n","      e.g.row_x: [ele_i,ele_j,ele_k]. norm2 = sqr(sum(ele_i^2+ele_i^2+ele_i^2))\n","      then divide each element by their row norm\n","      e.g. [ele_i/norm2,ele_j/norm2,ele_k/norm2] \n","    \"\"\"\n","    # 2-norm\n","    row_norm = sparse.linalg.norm(Z, axis = 1)\n","\n","    # row division to get the normalized Z\n","    diag = np.nan_to_num(1/row_norm)\n","    N = sparse.diags(diag,0)\n","    Z = N.dot(Z)\n","\n","    return Z\n","\n","  def edge_list_size(self, X):\n","    \"\"\"\n","      set default edge list size as S3.\n","      If find X only has 2 columns, \n","      return a flag \"S2\" indicating this is S2 edge list\n","    \"\"\"\n","    if X.shape[1] == 2:\n","      return \"S2\"\n","    else:\n","      return \"S3\"\n","    \n","  def Edge_to_Sparse(self, X, n, size_flag):\n","    \"\"\"\n","      input X is an edge list.\n","      For S2 edge list (e.g. node_i, node_j per row), add one to all connections\n","      return a sparse csr matrix of S3 edge list\n","    \"\"\"   \n","    #Build an empty sparse matrix. \n","    X_new = sparse.dok_matrix((n, n), dtype=np.float32)\n","\n","    for row in X:\n","      if size_flag == \"S2\":\n","        [node_i, node_j] = row\n","        X_new[node_i, node_j] = 1\n","      else:\n","        [node_i, node_j, weight] = row\n","        X_new[node_i, node_j] = weight\n","    \n","    X_new = sparse.csr_matrix(X_new)\n","\n","    return X_new\n","\n","\n","############------------graph_encoder_embed_end------------------###############"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["base_directory = \"C:/Users/user-pc/Dropbox/Papers/10_WorkInProgress/SoftwareNetworks/Data/snap-email/\"\n","edg_file = base_directory + \"email-eu-core.txt\"\n","node_file = base_directory + \"email-eu-core-department-labels.txt\"\n","\n","RlDataSet = RealDataSet(edg_file, node_file)\n","case10 = RlDataSet.get_initial_values()\n","test_case = case10.split_sets(0.2)\n"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"ename":"ValueError","evalue":"not enough values to unpack (expected 3, got 1)","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[1;32mIn [21], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m GEE \u001b[38;5;241m=\u001b[39m GraphEncoderEmbed()\n\u001b[0;32m      2\u001b[0m X_sparse \u001b[38;5;241m=\u001b[39m sparse\u001b[38;5;241m.\u001b[39mcsr_matrix(case10\u001b[38;5;241m.\u001b[39mX)\n\u001b[1;32m----> 3\u001b[0m Z_sparse, W \u001b[38;5;241m=\u001b[39m \u001b[43mGEE\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_sparse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcase10\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcase10\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEdgeList\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLaplacian\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDiagA\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCorrelation\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n","Cell \u001b[1;32mIn [7], line 12\u001b[0m, in \u001b[0;36mGraphEncoderEmbed.run\u001b[1;34m(self, X, Y, n, **kwargs)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEdgeList\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m     11\u001b[0m   size_flag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medge_list_size\n\u001b[1;32m---> 12\u001b[0m   X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEdge_to_Sparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize_flag\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDiagA\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m     15\u001b[0m   X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mDiagonal(X, n)\n","Cell \u001b[1;32mIn [7], line 141\u001b[0m, in \u001b[0;36mGraphEncoderEmbed.Edge_to_Sparse\u001b[1;34m(self, X, n, size_flag)\u001b[0m\n\u001b[0;32m    139\u001b[0m     X_new[node_i, node_j] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    140\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 141\u001b[0m     [node_i, node_j, weight] \u001b[38;5;241m=\u001b[39m row\n\u001b[0;32m    142\u001b[0m     X_new[node_i, node_j] \u001b[38;5;241m=\u001b[39m weight\n\u001b[0;32m    144\u001b[0m X_new \u001b[38;5;241m=\u001b[39m sparse\u001b[38;5;241m.\u001b[39mcsr_matrix(X_new)\n","\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 1)"]}],"source":["GEE = GraphEncoderEmbed()\n","Z_sparse, W = GEE.run(case10.X, case10.Y, case10.n, EdgeList = True, Laplacian = True, DiagA = True, Correlation = False)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["Dataset = DataPreprocess(case10, Laplacian = True, DiagA = False, emb_opt = \"AEE\")\n","Z, W = graph_encoder_embed(Dataset.X[0], Dataset.Y, Dataset.n, Correlation = False)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[[0.0000000e+00 1.0000000e+00 1.6502459e-02]\n"," [2.0000000e+00 3.0000000e+00 7.3088170e-03]\n"," [2.0000000e+00 4.0000000e+00 6.2112999e-03]\n"," ...\n"," [4.4000000e+02 4.6000000e+02 1.0023080e-02]\n"," [5.2000000e+01 7.8600000e+02 4.7088161e-02]\n"," [5.0600000e+02 9.3200000e+02 8.4095988e-03]]\n","[[ 0]\n"," [ 0]\n"," [20]\n"," ...\n"," [ 0]\n"," [ 5]\n"," [21]]\n"]}],"source":["print(Dataset.X[0])\n","print(Dataset.Y)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyM2SGgXl6aI5azKavI1scsr","collapsed_sections":[],"name":"Graph_Embed.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3.9.13 64-bit (windows store)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"vscode":{"interpreter":{"hash":"7212fb5c634bf58fb9c10f2b90edc507104e23d96b2947ef0ef4ca3b66cd6e5a"}}},"nbformat":4,"nbformat_minor":0}
